{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to base (Python 3.13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8dd036-b065-4f7e-84d5-635fca1f622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï¥ù ÌÉêÏÉâ Ï°∞Ìï© Ïàò: 10240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearch ÏßÑÌñâÏ§ë:   0%|          | 0/10240 [00:03<?, ?Ï°∞Ìï©/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 400, in _fit\n    self._validate_steps()\n    ~~~~~~~~~~~~~~~~~~~~^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 289, in _validate_steps\n    raise TypeError(\n        \"All intermediate steps of the chain should not be Pipelines\"\n    )\nTypeError: All intermediate steps of the chain should not be Pipelines\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 174\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m tqdm(param_list, desc=\u001b[33m\"\u001b[39m\u001b[33mGridSearch ÏßÑÌñâÏ§ë\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mÏ°∞Ìï©\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    173\u001b[39m     pipe.set_params(**params)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     results.append((params, scores.mean()))\n\u001b[32m    176\u001b[39m elapsed = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:419\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    401\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 400, in _fit\n    self._validate_steps()\n    ~~~~~~~~~~~~~~~~~~~~^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 289, in _validate_steps\n    raise TypeError(\n        \"All intermediate steps of the chain should not be Pipelines\"\n    )\nTypeError: All intermediate steps of the chain should not be Pipelines\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# ======================\n",
    "class DatetimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"date\" in df.columns and \"time\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(\n",
    "                df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n",
    "                errors=\"coerce\",\n",
    "                infer_datetime_format=True\n",
    "            )\n",
    "        df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "        df[\"shift\"] = df[\"hour\"].apply(lambda h: \"Day\" if 8 <= h <= 19 else \"Night\")\n",
    "        prev_count = df[\"count\"].iloc[0]\n",
    "        global_counts, accum = [], 0\n",
    "        for current_count in df[\"count\"]:\n",
    "            if current_count < prev_count:\n",
    "                accum += prev_count\n",
    "            global_counts.append(accum + current_count)\n",
    "            prev_count = current_count\n",
    "        df[\"global_count\"] = global_counts\n",
    "        df[\"year_month\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "        df[\"monthly_count\"] = df.groupby(\"year_month\").cumcount() + 1\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[\"speed_ratio\"] = df[\"low_section_speed\"] / df[\"high_section_speed\"]\n",
    "        df[\"pressure_speed_ratio\"] = df[\"cast_pressure\"] / df[\"high_section_speed\"]\n",
    "        df.loc[(df[\"low_section_speed\"] == 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[(df[\"low_section_speed\"] != 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[df[\"high_section_speed\"] == 0, \"pressure_speed_ratio\"] = -1\n",
    "        for col in [\"heating_furnace\", \"emergency_stop\", \"tryshot_signal\", \"EMS_operation_time\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "        if \"molten_temp\" in df.columns and df[\"molten_temp\"].isna().any():\n",
    "            df[\"molten_temp\"] = df[\"molten_temp\"].fillna(df[\"molten_temp\"].mode()[0])\n",
    "        if \"molten_volume\" in df.columns:\n",
    "            df[\"molten_volume\"] = df[\"molten_volume\"].interpolate(\"linear\").ffill().bfill()\n",
    "        df = df.replace([np.inf, -np.inf], -1)\n",
    "        return df\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols=None):\n",
    "        self.drop_cols = drop_cols or []\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=[c for c in self.drop_cols if c in X.columns])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Threshold Finder\n",
    "# ======================\n",
    "def find_best_threshold_fbeta(y_true, y_prob, beta=2.0):\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob)\n",
    "    t = np.append(t, 1.0)\n",
    "    fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(fbeta))\n",
    "    return float(t[best_idx]), float(fbeta[best_idx])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    test  = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "    y_train = train[\"passorfail\"]\n",
    "    X_train = train.drop(columns=[\"passorfail\"])\n",
    "    X_test  = test.copy()\n",
    "\n",
    "    # ----- Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "    drop_cols = [\"id\",\"line\",\"name\",\"mold_name\",\"date\",\"time\",\"registration_time\",\n",
    "                 \"year_month\",\"hour\",\"datetime\",\"real_time\",\"working\"]\n",
    "    base_preproc = SkPipeline(steps=[\n",
    "        (\"datetime\", DatetimeFeatureExtractor()),\n",
    "        (\"engineer\", FeatureEngineer()),\n",
    "        (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "    ])\n",
    "\n",
    "    tmp_after = base_preproc.fit_transform(X_train)\n",
    "    expected_cats = [\"mold_code\",\"heating_furnace\",\"EMS_operation_time\",\"shift\",\n",
    "                     \"emergency_stop\",\"tryshot_signal\"]\n",
    "    present_cats = [c for c in expected_cats if c in tmp_after.columns]\n",
    "\n",
    "    cat_pipe = SkPipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ])\n",
    "    num_pipe = SkPipeline(steps=[(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
    "    num_selector = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "    model_preproc = ColumnTransformer(\n",
    "        transformers=[(\"cat\", cat_pipe, present_cats),\n",
    "                      (\"num\", num_pipe, num_selector)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    categorical_feature_indices = list(range(len(present_cats)))\n",
    "\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"base\", base_preproc),\n",
    "        (\"prep\", model_preproc),\n",
    "        (\"smote\", SMOTENC(categorical_features=categorical_feature_indices, random_state=42)),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # ----- ÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î¶¨Îìú (Ï∂ïÏÜå Í∞ÄÎä•)\n",
    "    ratio_grid = [1/9, 2/8, 3/7, 4/6, 1.0]\n",
    "    param_grid = {\n",
    "        \"smote__sampling_strategy\": ratio_grid,\n",
    "        \"smote__k_neighbors\": [3, 5],\n",
    "        \"xgb__n_estimators\": [400, 800],\n",
    "        \"xgb__learning_rate\": [0.05, 0.08],\n",
    "        \"xgb__max_depth\": [4, 6],\n",
    "        \"xgb__min_child_weight\": [1, 3],\n",
    "        \"xgb__subsample\": [0.7, 1.0],\n",
    "        \"xgb__colsample_bytree\": [0.6, 1.0],\n",
    "        \"xgb__gamma\": [0.0, 0.1],\n",
    "        \"xgb__reg_lambda\": [1.0, 5.0],\n",
    "        \"xgb__reg_alpha\": [0.0, 0.5],\n",
    "        \"xgb__scale_pos_weight\": [1.0, 3.0]\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # ----- tqdm Grid Search (Recall Í∏∞Ï§Ä)\n",
    "    results = []\n",
    "    param_list = list(ParameterGrid(param_grid))\n",
    "    print(f\"Ï¥ù ÌÉêÏÉâ Ï°∞Ìï© Ïàò: {len(param_list)}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for params in tqdm(param_list, desc=\"GridSearch ÏßÑÌñâÏ§ë\", unit=\"Ï°∞Ìï©\"):\n",
    "        pipe.set_params(**params)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
    "        results.append((params, scores.mean()))\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # ----- ÏµúÏ†Å Ï°∞Ìï© ÏÑ†ÌÉù\n",
    "    best_params, best_recall = max(results, key=lambda x: x[1])\n",
    "    print(\"\\n===== GridSearchCV Í≤∞Í≥º (tqdm Í∏∞Î∞ò) =====\")\n",
    "    print(\"Best params :\", best_params)\n",
    "    print(\"Best Recall :\", best_recall)\n",
    "    print(f\"Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {elapsed/60:.2f} Î∂Ñ\")\n",
    "\n",
    "    # ----- ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\n",
    "    pipe.set_params(**best_params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ----- ÌôïÎ•† Î≥¥Ï†ï\n",
    "    calib = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=5)\n",
    "    calib.fit(X_train, y_train)\n",
    "\n",
    "    # ----- Threshold ÌÉêÏÉâ (F2 Í∏∞Ï§Ä)\n",
    "    train_proba = calib.predict_proba(X_train)[:, 1]\n",
    "    best_thr, best_f2 = find_best_threshold_fbeta(y_train.values, train_proba, beta=2.0)\n",
    "    print(f\"\\nBest threshold by F2 on train: {best_thr:.4f} (F2={best_f2:.4f})\")\n",
    "\n",
    "    # ----- ÌÖåÏä§Ìä∏ ÏòàÏ∏°\n",
    "    test_proba = calib.predict_proba(X_test)[:, 1]\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"passorfail\": test_proba}) \\\n",
    "        .to_csv(\"submission_best_xgb_proba.csv\", index=False)\n",
    "\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"prediction\": test_pred}) \\\n",
    "        .to_csv(\"submission_best_xgb_label.csv\", index=False)\n",
    "\n",
    "    print(\"\\nüìÅ Ï†ÄÏû• ÏôÑÎ£å: submission_best_xgb_proba.csv / submission_best_xgb_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c265ab5-aefb-45bd-92d6-c206b35b4b86",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-6d7a7207990f>, line 150)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 150\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mratio_grid = [1/9, 2/8, 3/7, 4/6, 1.0]\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# ======================\n",
    "class DatetimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"date\" in df.columns and \"time\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(\n",
    "                df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n",
    "                errors=\"coerce\",\n",
    "                infer_datetime_format=True\n",
    "            )\n",
    "        df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "        df[\"shift\"] = df[\"hour\"].apply(lambda h: \"Day\" if 8 <= h <= 19 else \"Night\")\n",
    "        prev_count = df[\"count\"].iloc[0]\n",
    "        global_counts, accum = [], 0\n",
    "        for current_count in df[\"count\"]:\n",
    "            if current_count < prev_count:\n",
    "                accum += prev_count\n",
    "            global_counts.append(accum + current_count)\n",
    "            prev_count = current_count\n",
    "        df[\"global_count\"] = global_counts\n",
    "        df[\"year_month\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "        df[\"monthly_count\"] = df.groupby(\"year_month\").cumcount() + 1\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[\"speed_ratio\"] = df[\"low_section_speed\"] / df[\"high_section_speed\"]\n",
    "        df[\"pressure_speed_ratio\"] = df[\"cast_pressure\"] / df[\"high_section_speed\"]\n",
    "        df.loc[(df[\"low_section_speed\"] == 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[(df[\"low_section_speed\"] != 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[df[\"high_section_speed\"] == 0, \"pressure_speed_ratio\"] = -1\n",
    "        for col in [\"heating_furnace\", \"emergency_stop\", \"tryshot_signal\", \"EMS_operation_time\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "        if \"molten_temp\" in df.columns and df[\"molten_temp\"].isna().any():\n",
    "            df[\"molten_temp\"] = df[\"molten_temp\"].fillna(df[\"molten_temp\"].mode()[0])\n",
    "        if \"molten_volume\" in df.columns:\n",
    "            df[\"molten_volume\"] = df[\"molten_volume\"].interpolate(\"linear\").ffill().bfill()\n",
    "        df = df.replace([np.inf, -np.inf], -1)\n",
    "        return df\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols=None):\n",
    "        self.drop_cols = drop_cols or []\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=[c for c in self.drop_cols if c in X.columns])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Threshold Finder\n",
    "# ======================\n",
    "def find_best_threshold_fbeta(y_true, y_prob, beta=2.0):\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob)\n",
    "    t = np.append(t, 1.0)\n",
    "    fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(fbeta))\n",
    "    return float(t[best_idx]), float(fbeta[best_idx])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    test  = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "    y_train = train[\"passorfail\"]\n",
    "    X_train = train.drop(columns=[\"passorfail\"])\n",
    "    X_test  = test.copy()\n",
    "\n",
    "    # ----- Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "    drop_cols = [\"id\",\"line\",\"name\",\"mold_name\",\"date\",\"time\",\"registration_time\",\n",
    "                 \"year_month\",\"hour\",\"datetime\",\"real_time\",\"working\"]\n",
    "    base_preproc = SkPipeline(steps=[\n",
    "        (\"datetime\", DatetimeFeatureExtractor()),\n",
    "        (\"engineer\", FeatureEngineer()),\n",
    "        (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "    ])\n",
    "\n",
    "    tmp_after = base_preproc.fit_transform(X_train)\n",
    "    expected_cats = [\"mold_code\",\"heating_furnace\",\"EMS_operation_time\",\"shift\",\n",
    "                     \"emergency_stop\",\"tryshot_signal\"]\n",
    "    present_cats = [c for c in expected_cats if c in tmp_after.columns]\n",
    "\n",
    "    cat_pipe = SkPipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ])\n",
    "    num_pipe = SkPipeline(steps=[(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
    "    num_selector = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "    model_preproc = ColumnTransformer(\n",
    "        transformers=[(\"cat\", cat_pipe, present_cats),\n",
    "                      (\"num\", num_pipe, num_selector)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    categorical_feature_indices = list(range(len(present_cats)))\n",
    "\n",
    "pipe = ImbPipeline(steps=[\n",
    "    (\"datetime\", DatetimeFeatureExtractor()),\n",
    "    (\"engineer\", FeatureEngineer()),\n",
    "    (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "    (\"prep\", model_preproc),\n",
    "    (\"smote\", SMOTENC(categorical_features=categorical_feature_indices, random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "    # ----- ÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î¶¨Îìú (Ï∂ïÏÜå Í∞ÄÎä•)\n",
    "    ratio_grid = [1/9, 2/8, 3/7, 4/6, 1.0]\n",
    "    param_grid = {\n",
    "        \"smote__sampling_strategy\": ratio_grid,\n",
    "        \"smote__k_neighbors\": [3, 5],\n",
    "        \"xgb__n_estimators\": [400, 800],\n",
    "        \"xgb__learning_rate\": [0.05, 0.08],\n",
    "        \"xgb__max_depth\": [4, 6],\n",
    "        \"xgb__min_child_weight\": [1, 3],\n",
    "        \"xgb__subsample\": [0.7, 1.0],\n",
    "        \"xgb__colsample_bytree\": [0.6, 1.0],\n",
    "        \"xgb__gamma\": [0.0, 0.1],\n",
    "        \"xgb__reg_lambda\": [1.0, 5.0],\n",
    "        \"xgb__reg_alpha\": [0.0, 0.5],\n",
    "        \"xgb__scale_pos_weight\": [1.0, 3.0]\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # ----- tqdm Grid Search (Recall Í∏∞Ï§Ä)\n",
    "    results = []\n",
    "    param_list = list(ParameterGrid(param_grid))\n",
    "    print(f\"Ï¥ù ÌÉêÏÉâ Ï°∞Ìï© Ïàò: {len(param_list)}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for params in tqdm(param_list, desc=\"GridSearch ÏßÑÌñâÏ§ë\", unit=\"Ï°∞Ìï©\"):\n",
    "        pipe.set_params(**params)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
    "        results.append((params, scores.mean()))\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # ----- ÏµúÏ†Å Ï°∞Ìï© ÏÑ†ÌÉù\n",
    "    best_params, best_recall = max(results, key=lambda x: x[1])\n",
    "    print(\"\\n===== GridSearchCV Í≤∞Í≥º (tqdm Í∏∞Î∞ò) =====\")\n",
    "    print(\"Best params :\", best_params)\n",
    "    print(\"Best Recall :\", best_recall)\n",
    "    print(f\"Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {elapsed/60:.2f} Î∂Ñ\")\n",
    "\n",
    "    # ----- ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\n",
    "    pipe.set_params(**best_params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ----- ÌôïÎ•† Î≥¥Ï†ï\n",
    "    calib = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=5)\n",
    "    calib.fit(X_train, y_train)\n",
    "\n",
    "    # ----- Threshold ÌÉêÏÉâ (F2 Í∏∞Ï§Ä)\n",
    "    train_proba = calib.predict_proba(X_train)[:, 1]\n",
    "    best_thr, best_f2 = find_best_threshold_fbeta(y_train.values, train_proba, beta=2.0)\n",
    "    print(f\"\\nBest threshold by F2 on train: {best_thr:.4f} (F2={best_f2:.4f})\")\n",
    "\n",
    "    # ----- ÌÖåÏä§Ìä∏ ÏòàÏ∏°\n",
    "    test_proba = calib.predict_proba(X_test)[:, 1]\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"passorfail\": test_proba}) \\\n",
    "        .to_csv(\"submission_best_xgb_proba.csv\", index=False)\n",
    "\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"prediction\": test_pred}) \\\n",
    "        .to_csv(\"submission_best_xgb_label.csv\", index=False)\n",
    "\n",
    "    print(\"\\nüìÅ Ï†ÄÏû• ÏôÑÎ£å: submission_best_xgb_proba.csv / submission_best_xgb_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4e94e-6753-4744-b31e-84dc1fa3eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï¥ù ÌÉêÏÉâ Ï°∞Ìï© Ïàò: 10240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearch ÏßÑÌñâÏ§ë:   1%|‚ñè         | 137/10240 [18:42<23:00:10,  8.20s/Ï°∞Ìï©]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m tqdm(param_list, desc=\u001b[33m\"\u001b[39m\u001b[33mGridSearch ÏßÑÌñâÏ§ë\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mÏ°∞Ìï©\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    175\u001b[39m     pipe.set_params(**params)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     results.append((params, scores.mean()))\n\u001b[32m    178\u001b[39m elapsed = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# ======================\n",
    "class DatetimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"date\" in df.columns and \"time\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(\n",
    "                df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n",
    "                errors=\"coerce\",\n",
    "                infer_datetime_format=True\n",
    "            )\n",
    "        df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "        df[\"shift\"] = df[\"hour\"].apply(lambda h: \"Day\" if 8 <= h <= 19 else \"Night\")\n",
    "        prev_count = df[\"count\"].iloc[0]\n",
    "        global_counts, accum = [], 0\n",
    "        for current_count in df[\"count\"]:\n",
    "            if current_count < prev_count:\n",
    "                accum += prev_count\n",
    "            global_counts.append(accum + current_count)\n",
    "            prev_count = current_count\n",
    "        df[\"global_count\"] = global_counts\n",
    "        df[\"year_month\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "        df[\"monthly_count\"] = df.groupby(\"year_month\").cumcount() + 1\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[\"speed_ratio\"] = df[\"low_section_speed\"] / df[\"high_section_speed\"]\n",
    "        df[\"pressure_speed_ratio\"] = df[\"cast_pressure\"] / df[\"high_section_speed\"]\n",
    "        df.loc[(df[\"low_section_speed\"] == 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[(df[\"low_section_speed\"] != 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[df[\"high_section_speed\"] == 0, \"pressure_speed_ratio\"] = -1\n",
    "        for col in [\"heating_furnace\", \"emergency_stop\", \"tryshot_signal\", \"EMS_operation_time\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "        if \"molten_temp\" in df.columns and df[\"molten_temp\"].isna().any():\n",
    "            df[\"molten_temp\"] = df[\"molten_temp\"].fillna(df[\"molten_temp\"].mode()[0])\n",
    "        if \"molten_volume\" in df.columns:\n",
    "            df[\"molten_volume\"] = df[\"molten_volume\"].interpolate(\"linear\").ffill().bfill()\n",
    "        df = df.replace([np.inf, -np.inf], -1)\n",
    "        return df\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols=None):\n",
    "        self.drop_cols = drop_cols or []\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=[c for c in self.drop_cols if c in X.columns])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Threshold Finder\n",
    "# ======================\n",
    "def find_best_threshold_fbeta(y_true, y_prob, beta=2.0):\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob)\n",
    "    t = np.append(t, 1.0)\n",
    "    fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(fbeta))\n",
    "    return float(t[best_idx]), float(fbeta[best_idx])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    test  = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "    y_train = train[\"passorfail\"]\n",
    "    X_train = train.drop(columns=[\"passorfail\"])\n",
    "    X_test  = test.copy()\n",
    "\n",
    "    # ----- Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨ ÌõÑ Ïª¨Îüº Ï†ïÏùò\n",
    "    drop_cols = [\"id\",\"line\",\"name\",\"mold_name\",\"date\",\"time\",\"registration_time\",\n",
    "                 \"year_month\",\"hour\",\"datetime\",\"real_time\",\"working\"]\n",
    "\n",
    "    tmp_after = (DatetimeFeatureExtractor()\n",
    "                 .fit_transform(X_train))\n",
    "    tmp_after = (FeatureEngineer()\n",
    "                 .fit_transform(tmp_after))\n",
    "    tmp_after = DropColumns(drop_cols=drop_cols).fit_transform(tmp_after)\n",
    "\n",
    "    expected_cats = [\"mold_code\",\"heating_furnace\",\"EMS_operation_time\",\"shift\",\n",
    "                     \"emergency_stop\",\"tryshot_signal\"]\n",
    "    present_cats = [c for c in expected_cats if c in tmp_after.columns]\n",
    "\n",
    "    cat_pipe = SkPipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ])\n",
    "    num_pipe = SkPipeline(steps=[(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
    "    num_selector = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "    model_preproc = ColumnTransformer(\n",
    "        transformers=[(\"cat\", cat_pipe, present_cats),\n",
    "                      (\"num\", num_pipe, num_selector)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    categorical_feature_indices = list(range(len(present_cats)))\n",
    "\n",
    "    # ----- ÏµúÏ¢Ö ÌååÏù¥ÌîÑÎùºÏù∏ (Ï§ëÏ≤© Pipeline Ï†úÍ±∞)\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"datetime\", DatetimeFeatureExtractor()),\n",
    "        (\"engineer\", FeatureEngineer()),\n",
    "        (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "        (\"prep\", model_preproc),\n",
    "        (\"smote\", SMOTENC(categorical_features=categorical_feature_indices, random_state=42)),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # ----- ÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î¶¨Îìú\n",
    "    ratio_grid = [1/9, 2/8, 3/7, 4/6, 1.0]\n",
    "    param_grid = {\n",
    "        \"smote__sampling_strategy\": ratio_grid,\n",
    "        \"smote__k_neighbors\": [3, 5],\n",
    "        \"xgb__n_estimators\": [400, 800],\n",
    "        \"xgb__learning_rate\": [0.05, 0.08],\n",
    "        \"xgb__max_depth\": [4, 6],\n",
    "        \"xgb__min_child_weight\": [1, 3],\n",
    "        \"xgb__subsample\": [0.7, 1.0],\n",
    "        \"xgb__colsample_bytree\": [0.6, 1.0],\n",
    "        \"xgb__gamma\": [0.0, 0.1],\n",
    "        \"xgb__reg_lambda\": [1.0, 5.0],\n",
    "        \"xgb__reg_alpha\": [0.0, 0.5],\n",
    "        \"xgb__scale_pos_weight\": [1.0, 3.0]\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # ----- tqdm Grid Search (Recall Í∏∞Ï§Ä)\n",
    "    results = []\n",
    "    param_list = list(ParameterGrid(param_grid))\n",
    "    print(f\"Ï¥ù ÌÉêÏÉâ Ï°∞Ìï© Ïàò: {len(param_list)}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for params in tqdm(param_list, desc=\"GridSearch ÏßÑÌñâÏ§ë\", unit=\"Ï°∞Ìï©\"):\n",
    "        pipe.set_params(**params)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
    "        results.append((params, scores.mean()))\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # ----- ÏµúÏ†Å Ï°∞Ìï© ÏÑ†ÌÉù\n",
    "    best_params, best_recall = max(results, key=lambda x: x[1])\n",
    "    print(\"\\n===== GridSearchCV Í≤∞Í≥º (tqdm Í∏∞Î∞ò) =====\")\n",
    "    print(\"Best params :\", best_params)\n",
    "    print(\"Best Recall :\", best_recall)\n",
    "    print(f\"Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {elapsed/60:.2f} Î∂Ñ\")\n",
    "\n",
    "    # ----- ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\n",
    "    pipe.set_params(**best_params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ----- ÌôïÎ•† Î≥¥Ï†ï\n",
    "    calib = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=5)\n",
    "    calib.fit(X_train, y_train)\n",
    "\n",
    "    # ----- Threshold ÌÉêÏÉâ (F2 Í∏∞Ï§Ä)\n",
    "    train_proba = calib.predict_proba(X_train)[:, 1]\n",
    "    best_thr, best_f2 = find_best_threshold_fbeta(y_train.values, train_proba, beta=2.0)\n",
    "    print(f\"\\nBest threshold by F2 on train: {best_thr:.4f} (F2={best_f2:.4f})\")\n",
    "\n",
    "    # ----- ÌÖåÏä§Ìä∏ ÏòàÏ∏°\n",
    "    test_proba = calib.predict_proba(X_test)[:, 1]\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"passorfail\": test_proba}) \\\n",
    "        .to_csv(\"submission_best_xgb_proba.csv\", index=False)\n",
    "\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"prediction\": test_pred}) \\\n",
    "        .to_csv(\"submission_best_xgb_label.csv\", index=False)\n",
    "\n",
    "    print(\"\\nüìÅ Ï†ÄÏû• ÏôÑÎ£å: submission_best_xgb_proba.csv / submission_best_xgb_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469b729-e317-4eee-a3b1-d79ab99307ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve, make_scorer, recall_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# ======================\n",
    "class DatetimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"date\" in df.columns and \"time\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(\n",
    "                df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n",
    "                errors=\"coerce\",\n",
    "                infer_datetime_format=True\n",
    "            )\n",
    "        df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "        df[\"shift\"] = df[\"hour\"].apply(lambda h: \"Day\" if 8 <= h <= 19 else \"Night\")\n",
    "        prev_count = df[\"count\"].iloc[0]\n",
    "        global_counts, accum = [], 0\n",
    "        for current_count in df[\"count\"]:\n",
    "            if current_count < prev_count:\n",
    "                accum += prev_count\n",
    "            global_counts.append(accum + current_count)\n",
    "            prev_count = current_count\n",
    "        df[\"global_count\"] = global_counts\n",
    "        df[\"year_month\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "        df[\"monthly_count\"] = df.groupby(\"year_month\").cumcount() + 1\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[\"speed_ratio\"] = df[\"low_section_speed\"] / df[\"high_section_speed\"]\n",
    "        df[\"pressure_speed_ratio\"] = df[\"cast_pressure\"] / df[\"high_section_speed\"]\n",
    "        df.loc[(df[\"low_section_speed\"] == 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[(df[\"low_section_speed\"] != 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[df[\"high_section_speed\"] == 0, \"pressure_speed_ratio\"] = -1\n",
    "        for col in [\"heating_furnace\", \"emergency_stop\", \"tryshot_signal\", \"EMS_operation_time\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "        if \"molten_temp\" in df.columns and df[\"molten_temp\"].isna().any():\n",
    "            df[\"molten_temp\"] = df[\"molten_temp\"].fillna(df[\"molten_temp\"].mode()[0])\n",
    "        if \"molten_volume\" in df.columns:\n",
    "            df[\"molten_volume\"] = df[\"molten_volume\"].interpolate(\"linear\").ffill().bfill()\n",
    "        df = df.replace([np.inf, -np.inf], -1)\n",
    "        return df\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols=None):\n",
    "        self.drop_cols = drop_cols or []\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=[c for c in self.drop_cols if c in X.columns])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Threshold Finder\n",
    "# ======================\n",
    "def find_best_threshold_fbeta(y_true, y_prob, beta=2.0):\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob)\n",
    "    t = np.append(t, 1.0)\n",
    "    fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(fbeta))\n",
    "    return float(t[best_idx]), float(fbeta[best_idx])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    test  = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "    y_train = train[\"passorfail\"]\n",
    "    X_train = train.drop(columns=[\"passorfail\"])\n",
    "    X_test  = test.copy()\n",
    "\n",
    "    # ----- Ïª¨Îüº Ï≤òÎ¶¨\n",
    "    drop_cols = [\"id\",\"line\",\"name\",\"mold_name\",\"date\",\"time\",\"registration_time\",\n",
    "                 \"year_month\",\"hour\",\"datetime\",\"real_time\",\"working\"]\n",
    "\n",
    "    tmp_after = DatetimeFeatureExtractor().fit_transform(X_train)\n",
    "    tmp_after = FeatureEngineer().fit_transform(tmp_after)\n",
    "    tmp_after = DropColumns(drop_cols=drop_cols).fit_transform(tmp_after)\n",
    "\n",
    "    expected_cats = [\"mold_code\",\"heating_furnace\",\"EMS_operation_time\",\"shift\",\n",
    "                     \"emergency_stop\",\"tryshot_signal\"]\n",
    "    present_cats = [c for c in expected_cats if c in tmp_after.columns]\n",
    "\n",
    "    cat_pipe = ColumnTransformer(\n",
    "        transformers=[(\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), present_cats)],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    categorical_feature_indices = list(range(len(present_cats)))\n",
    "\n",
    "    # ----- ÏµúÏ¢Ö ÌååÏù¥ÌîÑÎùºÏù∏\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"datetime\", DatetimeFeatureExtractor()),\n",
    "        (\"engineer\", FeatureEngineer()),\n",
    "        (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "        (\"prep\", cat_pipe),\n",
    "        (\"smote\", SMOTENC(categorical_features=categorical_feature_indices, random_state=42)),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # ======================\n",
    "    # Optuna Objective\n",
    "    # ======================\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"smote__sampling_strategy\": trial.suggest_float(\"smote__sampling_strategy\", 0.1, 1.0),\n",
    "            \"smote__k_neighbors\": trial.suggest_int(\"smote__k_neighbors\", 3, 7),\n",
    "            \"xgb__n_estimators\": trial.suggest_int(\"xgb__n_estimators\", 300, 1200),\n",
    "            \"xgb__learning_rate\": trial.suggest_float(\"xgb__learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"xgb__max_depth\": trial.suggest_int(\"xgb__max_depth\", 3, 8),\n",
    "            \"xgb__min_child_weight\": trial.suggest_int(\"xgb__min_child_weight\", 1, 5),\n",
    "            \"xgb__subsample\": trial.suggest_float(\"xgb__subsample\", 0.6, 1.0),\n",
    "            \"xgb__colsample_bytree\": trial.suggest_float(\"xgb__colsample_bytree\", 0.6, 1.0),\n",
    "            \"xgb__gamma\": trial.suggest_float(\"xgb__gamma\", 0.0, 0.3),\n",
    "            \"xgb__reg_lambda\": trial.suggest_float(\"xgb__reg_lambda\", 0.0, 5.0),\n",
    "            \"xgb__reg_alpha\": trial.suggest_float(\"xgb__reg_alpha\", 0.0, 1.0),\n",
    "            \"xgb__scale_pos_weight\": trial.suggest_float(\"xgb__scale_pos_weight\", 1.0, 5.0),\n",
    "        }\n",
    "        pipe.set_params(**params)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    # ======================\n",
    "    # Optuna Ïã§Ìñâ\n",
    "    # ======================\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=True)  # trial Ïàò Ï°∞Ï†à Í∞ÄÎä•\n",
    "\n",
    "    print(\"\\n===== Optuna Í≤∞Í≥º =====\")\n",
    "    print(\"Best Params:\", study.best_trial.params)\n",
    "    print(\"Best Recall:\", study.best_value)\n",
    "\n",
    "    # ----- Top 10 Trials\n",
    "    print(\"\\nTop 10 Trials (Recall Í∏∞Ï§Ä):\")\n",
    "    top_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:10]\n",
    "    for i, t in enumerate(top_trials, 1):\n",
    "        print(f\"Rank {i} | Recall={t.value:.4f} | Params={t.params}\")\n",
    "\n",
    "    # ----- ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\n",
    "    pipe.set_params(**study.best_trial.params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    calib = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=5)\n",
    "    calib.fit(X_train, y_train)\n",
    "\n",
    "    train_proba = calib.predict_proba(X_train)[:, 1]\n",
    "    best_thr, best_f2 = find_best_threshold_fbeta(y_train.values, train_proba, beta=2.0)\n",
    "    print(f\"\\nBest threshold by F2 on train: {best_thr:.4f} (F2={best_f2:.4f})\")\n",
    "\n",
    "    # ----- ÌÖåÏä§Ìä∏ ÏòàÏ∏°\n",
    "    test_proba = calib.predict_proba(X_test)[:, 1]\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"passorfail\": test_proba}) \\\n",
    "        .to_csv(\"submission_best_xgb_proba.csv\", index=False)\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"prediction\": test_pred}) \\\n",
    "        .to_csv(\"submission_best_xgb_label.csv\", index=False)\n",
    "\n",
    "    print(\"\\nüìÅ Ï†ÄÏû• ÏôÑÎ£å: submission_best_xgb_proba.csv / submission_best_xgb_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0170a1-3799-4e7a-aa2a-690acc1ae77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-30 16:13:02,341] A new study created in memory with name: no-name-528dbf85-ae65-4eb3-a3cf-9f1c9b24cd66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e426ff27b14aa48abadf9d9b9563d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-30 16:13:06,611] Trial 0 failed with parameters: {'smote__sampling_strategy': 0.786177799372898, 'smote__k_neighbors': 7, 'xgb__n_estimators': 633, 'xgb__learning_rate': 0.07975169607480741, 'xgb__max_depth': 3, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.9757404786443569, 'xgb__colsample_bytree': 0.9571795863020222, 'xgb__gamma': 0.2172406839150739, 'xgb__reg_lambda': 3.2023985514589084, 'xgb__reg_alpha': 0.898851173005305, 'xgb__scale_pos_weight': 4.576981515196467} because of the following error: ValueError('\\nAll the 5 fits failed.\\nIt is very likely that your model is misconfigured.\\nYou can try to debug the error by setting error_score=\\'raise\\'.\\n\\nBelow are more details about the failures:\\n--------------------------------------------------------------------------------\\n5 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py\", line 859, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\sklearn\\\\base.py\", line 1365, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\imblearn\\\\pipeline.py\", line 518, in fit\\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\\n             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\imblearn\\\\pipeline.py\", line 440, in _fit\\n    X, y, fitted_transformer = fit_resample_one_cached(\\n                               ~~~~~~~~~~~~~~~~~~~~~~~^\\n        cloned_transformer,\\n        ^^^^^^^^^^^^^^^^^^^\\n    ...<4 lines>...\\n        params=routed_params[name],\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\joblib\\\\memory.py\", line 326, in __call__\\n    return self.func(*args, **kwargs)\\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\imblearn\\\\pipeline.py\", line 1336, in _fit_resample_one\\n    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\\n                   ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\imblearn\\\\base.py\", line 202, in fit_resample\\n    return super().fit_resample(X, y, **params)\\n           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\sklearn\\\\base.py\", line 1365, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\imblearn\\\\base.py\", line 105, in fit_resample\\n    output = self._fit_resample(X, y, **params)\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\imblearn\\\\over_sampling\\\\_smote\\\\base.py\", line 599, in _fit_resample\\n    X_continuous = check_array(X_continuous, accept_sparse=[\"csr\", \"csc\"])\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\validation.py\", line 1105, in check_array\\n    _assert_all_finite(\\n    ~~~~~~~~~~~~~~~~~~^\\n        array,\\n        ^^^^^^\\n    ...<2 lines>...\\n        allow_nan=ensure_all_finite == \"allow-nan\",\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\validation.py\", line 120, in _assert_all_finite\\n    _assert_all_finite_element_wise(\\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\\n        X,\\n        ^^\\n    ...<4 lines>...\\n        input_name=input_name,\\n        ^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n  File \"c:\\\\Users\\\\kbk29\\\\miniconda3\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\validation.py\", line 169, in _assert_all_finite_element_wise\\n    raise ValueError(msg_err)\\nValueError: Input contains NaN.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-5-08f165a82b50>\", line 160, in objective\n",
      "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 677, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "        estimator=estimator,\n",
      "    ...<9 lines>...\n",
      "        error_score=error_score,\n",
      "    )\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 419, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 505, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n",
      "             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 440, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        params=routed_params[name],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1336, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\base.py\", line 202, in fit_resample\n",
      "    return super().fit_resample(X, y, **params)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\base.py\", line 105, in fit_resample\n",
      "    output = self._fit_resample(X, y, **params)\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 599, in _fit_resample\n",
      "    X_continuous = check_array(X_continuous, accept_sparse=[\"csr\", \"csc\"])\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1105, in check_array\n",
      "    _assert_all_finite(\n",
      "    ~~~~~~~~~~~~~~~~~~^\n",
      "        array,\n",
      "        ^^^^^^\n",
      "    ...<2 lines>...\n",
      "        allow_nan=ensure_all_finite == \"allow-nan\",\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 120, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        X,\n",
      "        ^^\n",
      "    ...<4 lines>...\n",
      "        input_name=input_name,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 169, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "[W 2025-09-30 16:13:06,616] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 440, in _fit\n    X, y, fitted_transformer = fit_resample_one_cached(\n                               ~~~~~~~~~~~~~~~~~~~~~~~^\n        cloned_transformer,\n        ^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n        params=routed_params[name],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1336, in _fit_resample_one\n    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n                   ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\base.py\", line 202, in fit_resample\n    return super().fit_resample(X, y, **params)\n           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\base.py\", line 105, in fit_resample\n    output = self._fit_resample(X, y, **params)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 599, in _fit_resample\n    X_continuous = check_array(X_continuous, accept_sparse=[\"csr\", \"csc\"])\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1105, in check_array\n    _assert_all_finite(\n    ~~~~~~~~~~~~~~~~~~^\n        array,\n        ^^^^^^\n    ...<2 lines>...\n        allow_nan=ensure_all_finite == \"allow-nan\",\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        X,\n        ^^\n    ...<4 lines>...\n        input_name=input_name,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input contains NaN.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 167\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Optuna Ïã§Ìñâ\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[32m    166\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# trial Ïàò Ï°∞Ï†à Í∞ÄÎä•\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===== Optuna Í≤∞Í≥º =====\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest Params:\u001b[39m\u001b[33m\"\u001b[39m, study.best_trial.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    145\u001b[39m params = {\n\u001b[32m    146\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msmote__sampling_strategy\u001b[39m\u001b[33m\"\u001b[39m: trial.suggest_float(\u001b[33m\"\u001b[39m\u001b[33msmote__sampling_strategy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1.0\u001b[39m),\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msmote__k_neighbors\u001b[39m\u001b[33m\"\u001b[39m: trial.suggest_int(\u001b[33m\"\u001b[39m\u001b[33msmote__k_neighbors\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m7\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mxgb__scale_pos_weight\u001b[39m\u001b[33m\"\u001b[39m: trial.suggest_float(\u001b[33m\"\u001b[39m\u001b[33mxgb__scale_pos_weight\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m5.0\u001b[39m),\n\u001b[32m    158\u001b[39m }\n\u001b[32m    159\u001b[39m pipe.set_params(**params)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:419\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    401\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 440, in _fit\n    X, y, fitted_transformer = fit_resample_one_cached(\n                               ~~~~~~~~~~~~~~~~~~~~~~~^\n        cloned_transformer,\n        ^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n        params=routed_params[name],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1336, in _fit_resample_one\n    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n                   ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\base.py\", line 202, in fit_resample\n    return super().fit_resample(X, y, **params)\n           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\base.py\", line 105, in fit_resample\n    output = self._fit_resample(X, y, **params)\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 599, in _fit_resample\n    X_continuous = check_array(X_continuous, accept_sparse=[\"csr\", \"csc\"])\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1105, in check_array\n    _assert_all_finite(\n    ~~~~~~~~~~~~~~~~~~^\n        array,\n        ^^^^^^\n    ...<2 lines>...\n        allow_nan=ensure_all_finite == \"allow-nan\",\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        X,\n        ^^\n    ...<4 lines>...\n        input_name=input_name,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\kbk29\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input contains NaN.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve, make_scorer, recall_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# ======================\n",
    "class DatetimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"date\" in df.columns and \"time\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(\n",
    "                df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n",
    "                errors=\"coerce\",\n",
    "                infer_datetime_format=True\n",
    "            )\n",
    "        df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "        df[\"shift\"] = df[\"hour\"].apply(lambda h: \"Day\" if 8 <= h <= 19 else \"Night\")\n",
    "        prev_count = df[\"count\"].iloc[0]\n",
    "        global_counts, accum = [], 0\n",
    "        for current_count in df[\"count\"]:\n",
    "            if current_count < prev_count:\n",
    "                accum += prev_count\n",
    "            global_counts.append(accum + current_count)\n",
    "            prev_count = current_count\n",
    "        df[\"global_count\"] = global_counts\n",
    "        df[\"year_month\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "        df[\"monthly_count\"] = df.groupby(\"year_month\").cumcount() + 1\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[\"speed_ratio\"] = df[\"low_section_speed\"] / df[\"high_section_speed\"]\n",
    "        df[\"pressure_speed_ratio\"] = df[\"cast_pressure\"] / df[\"high_section_speed\"]\n",
    "        df.loc[(df[\"low_section_speed\"] == 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[(df[\"low_section_speed\"] != 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[df[\"high_section_speed\"] == 0, \"pressure_speed_ratio\"] = -1\n",
    "        for col in [\"heating_furnace\", \"emergency_stop\", \"tryshot_signal\", \"EMS_operation_time\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "        if \"molten_temp\" in df.columns and df[\"molten_temp\"].isna().any():\n",
    "            df[\"molten_temp\"] = df[\"molten_temp\"].fillna(df[\"molten_temp\"].mode()[0])\n",
    "        if \"molten_volume\" in df.columns:\n",
    "            df[\"molten_volume\"] = df[\"molten_volume\"].interpolate(\"linear\").ffill().bfill()\n",
    "        df = df.replace([np.inf, -np.inf], -1)\n",
    "        return df\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols=None):\n",
    "        self.drop_cols = drop_cols or []\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=[c for c in self.drop_cols if c in X.columns])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Threshold Finder\n",
    "# ======================\n",
    "def find_best_threshold_fbeta(y_true, y_prob, beta=2.0):\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob)\n",
    "    t = np.append(t, 1.0)\n",
    "    fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(fbeta))\n",
    "    return float(t[best_idx]), float(fbeta[best_idx])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    test  = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "    y_train = train[\"passorfail\"]\n",
    "    X_train = train.drop(columns=[\"passorfail\"])\n",
    "    X_test  = test.copy()\n",
    "\n",
    "    # ----- Ïª¨Îüº Ï≤òÎ¶¨\n",
    "    drop_cols = [\"id\",\"line\",\"name\",\"mold_name\",\"date\",\"time\",\"registration_time\",\n",
    "                 \"year_month\",\"hour\",\"datetime\",\"real_time\",\"working\"]\n",
    "\n",
    "    tmp_after = DatetimeFeatureExtractor().fit_transform(X_train)\n",
    "    tmp_after = FeatureEngineer().fit_transform(tmp_after)\n",
    "    tmp_after = DropColumns(drop_cols=drop_cols).fit_transform(tmp_after)\n",
    "\n",
    "    expected_cats = [\"mold_code\",\"heating_furnace\",\"EMS_operation_time\",\"shift\",\n",
    "                     \"emergency_stop\",\"tryshot_signal\"]\n",
    "    present_cats = [c for c in expected_cats if c in tmp_after.columns]\n",
    "\n",
    "    cat_pipe = ColumnTransformer(\n",
    "        transformers=[(\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), present_cats)],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    categorical_feature_indices = list(range(len(present_cats)))\n",
    "\n",
    "    # ----- ÏµúÏ¢Ö ÌååÏù¥ÌîÑÎùºÏù∏\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"datetime\", DatetimeFeatureExtractor()),\n",
    "        (\"engineer\", FeatureEngineer()),\n",
    "        (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "        (\"prep\", cat_pipe),\n",
    "        (\"smote\", SMOTENC(categorical_features=categorical_feature_indices, random_state=42)),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # ======================\n",
    "    # Optuna Objective\n",
    "    # ======================\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"smote__sampling_strategy\": trial.suggest_float(\"smote__sampling_strategy\", 0.1, 1.0),\n",
    "            \"smote__k_neighbors\": trial.suggest_int(\"smote__k_neighbors\", 3, 7),\n",
    "            \"xgb__n_estimators\": trial.suggest_int(\"xgb__n_estimators\", 300, 1200),\n",
    "            \"xgb__learning_rate\": trial.suggest_float(\"xgb__learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"xgb__max_depth\": trial.suggest_int(\"xgb__max_depth\", 3, 8),\n",
    "            \"xgb__min_child_weight\": trial.suggest_int(\"xgb__min_child_weight\", 1, 5),\n",
    "            \"xgb__subsample\": trial.suggest_float(\"xgb__subsample\", 0.6, 1.0),\n",
    "            \"xgb__colsample_bytree\": trial.suggest_float(\"xgb__colsample_bytree\", 0.6, 1.0),\n",
    "            \"xgb__gamma\": trial.suggest_float(\"xgb__gamma\", 0.0, 0.3),\n",
    "            \"xgb__reg_lambda\": trial.suggest_float(\"xgb__reg_lambda\", 0.0, 5.0),\n",
    "            \"xgb__reg_alpha\": trial.suggest_float(\"xgb__reg_alpha\", 0.0, 1.0),\n",
    "            \"xgb__scale_pos_weight\": trial.suggest_float(\"xgb__scale_pos_weight\", 1.0, 5.0),\n",
    "        }\n",
    "        pipe.set_params(**params)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    # ======================\n",
    "    # Optuna Ïã§Ìñâ\n",
    "    # ======================\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=True)  # trial Ïàò Ï°∞Ï†à Í∞ÄÎä•\n",
    "\n",
    "    print(\"\\n===== Optuna Í≤∞Í≥º =====\")\n",
    "    print(\"Best Params:\", study.best_trial.params)\n",
    "    print(\"Best Recall:\", study.best_value)\n",
    "\n",
    "    # ----- Top 10 Trials\n",
    "    print(\"\\nTop 10 Trials (Recall Í∏∞Ï§Ä):\")\n",
    "    top_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:10]\n",
    "    for i, t in enumerate(top_trials, 1):\n",
    "        print(f\"Rank {i} | Recall={t.value:.4f} | Params={t.params}\")\n",
    "\n",
    "    # ----- ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\n",
    "    pipe.set_params(**study.best_trial.params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    calib = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=5)\n",
    "    calib.fit(X_train, y_train)\n",
    "\n",
    "    train_proba = calib.predict_proba(X_train)[:, 1]\n",
    "    best_thr, best_f2 = find_best_threshold_fbeta(y_train.values, train_proba, beta=2.0)\n",
    "    print(f\"\\nBest threshold by F2 on train: {best_thr:.4f} (F2={best_f2:.4f})\")\n",
    "\n",
    "    # ----- ÌÖåÏä§Ìä∏ ÏòàÏ∏°\n",
    "    test_proba = calib.predict_proba(X_test)[:, 1]\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"passorfail\": test_proba}) \\\n",
    "        .to_csv(\"submission_best_xgb_proba.csv\", index=False)\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "    pd.DataFrame({\"id\": X_test[\"id\"], \"prediction\": test_pred}) \\\n",
    "        .to_csv(\"submission_best_xgb_label.csv\", index=False)\n",
    "\n",
    "    print(\"\\nüìÅ Ï†ÄÏû• ÏôÑÎ£å: submission_best_xgb_proba.csv / submission_best_xgb_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350397fa-9f16-4019-892f-5ad84b0699d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-30 16:14:46,424] A new study created in memory with name: no-name-75caa3db-38ce-43f9-b020-78cb700a05e6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffb432829cd4f96b8d1a4e4a4de1502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-30 16:14:57,758] Trial 0 finished with value: 0.9490695401228821 and parameters: {'smote__sampling_strategy': 0.2751606068792357, 'smote__k_neighbors': 7, 'xgb__n_estimators': 658, 'xgb__learning_rate': 0.04155214197703763, 'xgb__max_depth': 3, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.9597330990994736, 'xgb__colsample_bytree': 0.8650535461923865, 'xgb__gamma': 0.06963277657738065, 'xgb__reg_lambda': 2.3731210998357315, 'xgb__reg_alpha': 0.4038995452618023, 'xgb__scale_pos_weight': 3.7153452192257874}. Best is trial 0 with value: 0.9490695401228821.\n",
      "[I 2025-09-30 16:15:12,153] Trial 1 finished with value: 0.9469377210947683 and parameters: {'smote__sampling_strategy': 0.4445858190561319, 'smote__k_neighbors': 7, 'xgb__n_estimators': 634, 'xgb__learning_rate': 0.1381954834916881, 'xgb__max_depth': 7, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.8806395872838246, 'xgb__colsample_bytree': 0.5633127853482096, 'xgb__gamma': 0.10272571012008141, 'xgb__reg_lambda': 2.6523195235622796, 'xgb__reg_alpha': 0.2362288308502325, 'xgb__scale_pos_weight': 3.5802936508572727}. Best is trial 0 with value: 0.9490695401228821.\n",
      "[I 2025-09-30 16:15:28,545] Trial 2 finished with value: 0.9393134425619065 and parameters: {'smote__sampling_strategy': 0.5731046286767895, 'smote__k_neighbors': 7, 'xgb__n_estimators': 747, 'xgb__learning_rate': 0.010347194056738987, 'xgb__max_depth': 5, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.7505271189273316, 'xgb__colsample_bytree': 0.6227335997869903, 'xgb__gamma': 0.4881717119250991, 'xgb__reg_lambda': 0.6156506528349545, 'xgb__reg_alpha': 0.8482113430845524, 'xgb__scale_pos_weight': 1.2988516892419892}. Best is trial 0 with value: 0.9490695401228821.\n",
      "[I 2025-09-30 16:15:47,210] Trial 3 finished with value: 0.9444968348538447 and parameters: {'smote__sampling_strategy': 0.9129220016006733, 'smote__k_neighbors': 3, 'xgb__n_estimators': 768, 'xgb__learning_rate': 0.04728586305028638, 'xgb__max_depth': 8, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.9758328969515776, 'xgb__colsample_bytree': 0.7909818223070022, 'xgb__gamma': 0.08223575340515732, 'xgb__reg_lambda': 1.197553579535081, 'xgb__reg_alpha': 0.1798033539837074, 'xgb__scale_pos_weight': 1.2483259489343648}. Best is trial 0 with value: 0.9490695401228821.\n",
      "[I 2025-09-30 16:15:56,087] Trial 4 finished with value: 0.9341277229566188 and parameters: {'smote__sampling_strategy': 0.5969736222421704, 'smote__k_neighbors': 5, 'xgb__n_estimators': 332, 'xgb__learning_rate': 0.013995585152116102, 'xgb__max_depth': 4, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.6687688934407718, 'xgb__colsample_bytree': 0.8314426900890743, 'xgb__gamma': 0.3998683375450553, 'xgb__reg_lambda': 4.262304582236822, 'xgb__reg_alpha': 0.5300065238060883, 'xgb__scale_pos_weight': 1.2545246523717735}. Best is trial 0 with value: 0.9490695401228821.\n",
      "[I 2025-09-30 16:16:07,721] Trial 5 finished with value: 0.9493767454850122 and parameters: {'smote__sampling_strategy': 0.9154699811443945, 'smote__k_neighbors': 5, 'xgb__n_estimators': 366, 'xgb__learning_rate': 0.06859904435809334, 'xgb__max_depth': 6, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.8597131632788024, 'xgb__colsample_bytree': 0.7180190430623743, 'xgb__gamma': 0.4043279980824746, 'xgb__reg_lambda': 0.9736352645422023, 'xgb__reg_alpha': 0.8776309448348503, 'xgb__scale_pos_weight': 2.054045018016207}. Best is trial 5 with value: 0.9493767454850122.\n",
      "[I 2025-09-30 16:16:21,601] Trial 6 finished with value: 0.946631912120648 and parameters: {'smote__sampling_strategy': 0.47103996505080303, 'smote__k_neighbors': 5, 'xgb__n_estimators': 780, 'xgb__learning_rate': 0.12483116895711306, 'xgb__max_depth': 7, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.534194484266568, 'xgb__colsample_bytree': 0.8646893918532659, 'xgb__gamma': 0.4053311928482683, 'xgb__reg_lambda': 0.7141601800876125, 'xgb__reg_alpha': 0.4840372656822072, 'xgb__scale_pos_weight': 3.471504529407244}. Best is trial 5 with value: 0.9493767454850122.\n",
      "[I 2025-09-30 16:16:30,643] Trial 7 finished with value: 0.9243706944703035 and parameters: {'smote__sampling_strategy': 0.12043716969575165, 'smote__k_neighbors': 7, 'xgb__n_estimators': 607, 'xgb__learning_rate': 0.015183361427415902, 'xgb__max_depth': 4, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.645346233012169, 'xgb__colsample_bytree': 0.9936973732848072, 'xgb__gamma': 0.49009703292873835, 'xgb__reg_lambda': 1.503382819172846, 'xgb__reg_alpha': 0.7136781592304663, 'xgb__scale_pos_weight': 1.091643344045632}. Best is trial 5 with value: 0.9493767454850122.\n",
      "[I 2025-09-30 16:16:42,148] Trial 8 finished with value: 0.9606576987525599 and parameters: {'smote__sampling_strategy': 0.7635267654184624, 'smote__k_neighbors': 4, 'xgb__n_estimators': 471, 'xgb__learning_rate': 0.016865534469100985, 'xgb__max_depth': 4, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.8055320008790773, 'xgb__colsample_bytree': 0.7498985800550794, 'xgb__gamma': 0.33127980488119496, 'xgb__reg_lambda': 1.7832856227512923, 'xgb__reg_alpha': 0.047395337497305734, 'xgb__scale_pos_weight': 4.27956195092046}. Best is trial 8 with value: 0.9606576987525599.\n",
      "[I 2025-09-30 16:16:55,442] Trial 9 finished with value: 0.9423617575870414 and parameters: {'smote__sampling_strategy': 0.564092643830766, 'smote__k_neighbors': 5, 'xgb__n_estimators': 785, 'xgb__learning_rate': 0.03939902136451284, 'xgb__max_depth': 3, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.7304222102481706, 'xgb__colsample_bytree': 0.5522832596719147, 'xgb__gamma': 0.2668055750554415, 'xgb__reg_lambda': 1.009934011034374, 'xgb__reg_alpha': 0.6407886831364382, 'xgb__scale_pos_weight': 1.66072623060727}. Best is trial 8 with value: 0.9606576987525599.\n",
      "[I 2025-09-30 16:17:07,356] Trial 10 finished with value: 0.9634011357289145 and parameters: {'smote__sampling_strategy': 0.769486427420692, 'smote__k_neighbors': 3, 'xgb__n_estimators': 447, 'xgb__learning_rate': 0.02328154552808944, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8198855682650426, 'xgb__colsample_bytree': 0.6587533850290036, 'xgb__gamma': 0.2199559892151777, 'xgb__reg_lambda': 2.2120039847125437, 'xgb__reg_alpha': 0.10551744616035731, 'xgb__scale_pos_weight': 4.419781154392707}. Best is trial 10 with value: 0.9634011357289145.\n",
      "[I 2025-09-30 16:17:19,072] Trial 11 finished with value: 0.9643162353379259 and parameters: {'smote__sampling_strategy': 0.7725164093513868, 'smote__k_neighbors': 3, 'xgb__n_estimators': 455, 'xgb__learning_rate': 0.02311736795959936, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8185197377844964, 'xgb__colsample_bytree': 0.6857768506727961, 'xgb__gamma': 0.21701344916614496, 'xgb__reg_lambda': 2.4283127317410798, 'xgb__reg_alpha': 0.06611769764790727, 'xgb__scale_pos_weight': 4.5217303229157935}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:17:31,145] Trial 12 finished with value: 0.9640122882144851 and parameters: {'smote__sampling_strategy': 0.7596745040621485, 'smote__k_neighbors': 3, 'xgb__n_estimators': 467, 'xgb__learning_rate': 0.025484132975590136, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8604854416532041, 'xgb__colsample_bytree': 0.6601468936545319, 'xgb__gamma': 0.1801862854881622, 'xgb__reg_lambda': 3.938181975863292, 'xgb__reg_alpha': 0.009885698950640384, 'xgb__scale_pos_weight': 4.68106127387665}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:17:45,789] Trial 13 finished with value: 0.9624874325079128 and parameters: {'smote__sampling_strategy': 0.7621469661746347, 'smote__k_neighbors': 3, 'xgb__n_estimators': 499, 'xgb__learning_rate': 0.027422033658374218, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.9089941741843995, 'xgb__colsample_bytree': 0.6734300120080244, 'xgb__gamma': 0.18211832764964273, 'xgb__reg_lambda': 4.9224671141686995, 'xgb__reg_alpha': 0.3177365836806329, 'xgb__scale_pos_weight': 4.955343855203502}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:18:00,259] Trial 14 finished with value: 0.9588274995345374 and parameters: {'smote__sampling_strategy': 0.9981925200868568, 'smote__k_neighbors': 4, 'xgb__n_estimators': 384, 'xgb__learning_rate': 0.02235557607338104, 'xgb__max_depth': 5, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.7687166931563091, 'xgb__colsample_bytree': 0.6131783489532551, 'xgb__gamma': 0.15839623277862536, 'xgb__reg_lambda': 3.5832597928400887, 'xgb__reg_alpha': 0.009080701933187607, 'xgb__scale_pos_weight': 2.6469928502346725}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:18:14,660] Trial 15 finished with value: 0.9515099609011358 and parameters: {'smote__sampling_strategy': 0.7012294955402768, 'smote__k_neighbors': 4, 'xgb__n_estimators': 556, 'xgb__learning_rate': 0.07959313747909692, 'xgb__max_depth': 6, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.9203240367658216, 'xgb__colsample_bytree': 0.6991025163150687, 'xgb__gamma': 0.2923605021687922, 'xgb__reg_lambda': 3.3264066768138276, 'xgb__reg_alpha': 0.23266444605987557, 'xgb__scale_pos_weight': 4.948020376193714}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:18:25,406] Trial 16 finished with value: 0.9615718674362317 and parameters: {'smote__sampling_strategy': 0.6709076920911883, 'smote__k_neighbors': 3, 'xgb__n_estimators': 412, 'xgb__learning_rate': 0.029871536896459126, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8344100077460399, 'xgb__colsample_bytree': 0.5351679714728601, 'xgb__gamma': 0.005444796671138008, 'xgb__reg_lambda': 3.228727563494379, 'xgb__reg_alpha': 0.13568090268690425, 'xgb__scale_pos_weight': 4.270532165559594}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:18:37,242] Trial 17 finished with value: 0.9521206479240366 and parameters: {'smote__sampling_strategy': 0.8754441253453177, 'smote__k_neighbors': 6, 'xgb__n_estimators': 303, 'xgb__learning_rate': 0.06469814811417465, 'xgb__max_depth': 7, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.684988711072838, 'xgb__colsample_bytree': 0.6049064900268926, 'xgb__gamma': 0.15193836792999926, 'xgb__reg_lambda': 1.9612562247877419, 'xgb__reg_alpha': 0.32981676716674535, 'xgb__scale_pos_weight': 2.923401094129748}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:18:48,872] Trial 18 finished with value: 0.9570000930925339 and parameters: {'smote__sampling_strategy': 0.43723154496742916, 'smote__k_neighbors': 4, 'xgb__n_estimators': 543, 'xgb__learning_rate': 0.019366446649662653, 'xgb__max_depth': 5, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.5803173567830138, 'xgb__colsample_bytree': 0.5048878664290863, 'xgb__gamma': 0.23980367174062012, 'xgb__reg_lambda': 2.783921061032611, 'xgb__reg_alpha': 0.017502386667503977, 'xgb__scale_pos_weight': 3.9408356792011197}. Best is trial 11 with value: 0.9643162353379259.\n",
      "[I 2025-09-30 16:19:03,446] Trial 19 finished with value: 0.9664531744554086 and parameters: {'smote__sampling_strategy': 0.8720679052176163, 'smote__k_neighbors': 3, 'xgb__n_estimators': 536, 'xgb__learning_rate': 0.011309626065281651, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7817757557752542, 'xgb__colsample_bytree': 0.7610341369684775, 'xgb__gamma': 0.317690601991742, 'xgb__reg_lambda': 1.4536900190258994, 'xgb__reg_alpha': 0.30245181417777384, 'xgb__scale_pos_weight': 4.511464146520451}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:19:19,481] Trial 20 finished with value: 0.9557791845094024 and parameters: {'smote__sampling_strategy': 0.8587885990723638, 'smote__k_neighbors': 4, 'xgb__n_estimators': 532, 'xgb__learning_rate': 0.011521540782769344, 'xgb__max_depth': 8, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.7102787478409663, 'xgb__colsample_bytree': 0.9355352760978198, 'xgb__gamma': 0.33951887931211316, 'xgb__reg_lambda': 1.4593635742394397, 'xgb__reg_alpha': 0.36429352383486, 'xgb__scale_pos_weight': 3.1228414462924}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:19:34,063] Trial 21 finished with value: 0.962486967045243 and parameters: {'smote__sampling_strategy': 0.9908958863782173, 'smote__k_neighbors': 3, 'xgb__n_estimators': 434, 'xgb__learning_rate': 0.031472549301842824, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7876812385288898, 'xgb__colsample_bytree': 0.7538087539818991, 'xgb__gamma': 0.20744759598328177, 'xgb__reg_lambda': 4.099777562677025, 'xgb__reg_alpha': 0.1296478115304034, 'xgb__scale_pos_weight': 4.632066123285275}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:19:47,538] Trial 22 finished with value: 0.962793706944703 and parameters: {'smote__sampling_strategy': 0.8079679308783605, 'smote__k_neighbors': 3, 'xgb__n_estimators': 501, 'xgb__learning_rate': 0.012553937208209591, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.867869082196751, 'xgb__colsample_bytree': 0.7873230193029374, 'xgb__gamma': 0.3029439000631817, 'xgb__reg_lambda': 1.2211108676659266, 'xgb__reg_alpha': 0.21846278180988632, 'xgb__scale_pos_weight': 3.999738205687625}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:20:01,346] Trial 23 finished with value: 0.9609625768013406 and parameters: {'smote__sampling_strategy': 0.6763883288736235, 'smote__k_neighbors': 3, 'xgb__n_estimators': 599, 'xgb__learning_rate': 0.01867668771361378, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.925180940573357, 'xgb__colsample_bytree': 0.6677484106364082, 'xgb__gamma': 0.12280132401119598, 'xgb__reg_lambda': 1.8685724951903842, 'xgb__reg_alpha': 0.08298131567555411, 'xgb__scale_pos_weight': 4.631452049078057}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:20:18,615] Trial 24 finished with value: 0.9582196052876558 and parameters: {'smote__sampling_strategy': 0.8337692717014145, 'smote__k_neighbors': 4, 'xgb__n_estimators': 699, 'xgb__learning_rate': 0.024589254590114747, 'xgb__max_depth': 7, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.8344861714081643, 'xgb__colsample_bytree': 0.7193611696032579, 'xgb__gamma': 0.25063112062659687, 'xgb__reg_lambda': 2.536085106215578, 'xgb__reg_alpha': 0.27858149295442364, 'xgb__scale_pos_weight': 4.706122909548463}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:20:30,281] Trial 25 finished with value: 0.9588307577732266 and parameters: {'smote__sampling_strategy': 0.6792309728384116, 'smote__k_neighbors': 6, 'xgb__n_estimators': 476, 'xgb__learning_rate': 0.010098752843231086, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7717712867354772, 'xgb__colsample_bytree': 0.7630669474463107, 'xgb__gamma': 0.3504365898262669, 'xgb__reg_lambda': 2.9092992516354355, 'xgb__reg_alpha': 0.4501080233181344, 'xgb__scale_pos_weight': 4.0258434980700315}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:20:44,451] Trial 26 finished with value: 0.9643171662632657 and parameters: {'smote__sampling_strategy': 0.9349318927976972, 'smote__k_neighbors': 3, 'xgb__n_estimators': 569, 'xgb__learning_rate': 0.035695536700790745, 'xgb__max_depth': 4, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.6372443836612128, 'xgb__colsample_bytree': 0.6350485140990396, 'xgb__gamma': 0.19737583329506736, 'xgb__reg_lambda': 3.8331170255513443, 'xgb__reg_alpha': 0.5874044485123946, 'xgb__scale_pos_weight': 4.995667617332649}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:20:58,420] Trial 27 finished with value: 0.9515085645131259 and parameters: {'smote__sampling_strategy': 0.9354268637694156, 'smote__k_neighbors': 4, 'xgb__n_estimators': 577, 'xgb__learning_rate': 0.05518281109321565, 'xgb__max_depth': 3, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.5901616337680966, 'xgb__colsample_bytree': 0.8192533405744442, 'xgb__gamma': 0.29939747774258607, 'xgb__reg_lambda': 2.1023487816468043, 'xgb__reg_alpha': 0.5834577505063758, 'xgb__scale_pos_weight': 2.378533675353263}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:21:13,855] Trial 28 finished with value: 0.963706013777695 and parameters: {'smote__sampling_strategy': 0.9669131610162475, 'smote__k_neighbors': 3, 'xgb__n_estimators': 521, 'xgb__learning_rate': 0.03402134900072943, 'xgb__max_depth': 4, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.6343370273674797, 'xgb__colsample_bytree': 0.5799192204707221, 'xgb__gamma': 0.206562774816221, 'xgb__reg_lambda': 4.965013957087807, 'xgb__reg_alpha': 0.7342225126578525, 'xgb__scale_pos_weight': 4.33286884459314}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:21:29,122] Trial 29 finished with value: 0.9545582759262707 and parameters: {'smote__sampling_strategy': 0.8882096156220455, 'smote__k_neighbors': 6, 'xgb__n_estimators': 639, 'xgb__learning_rate': 0.03958582481739431, 'xgb__max_depth': 4, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.7116795129669601, 'xgb__colsample_bytree': 0.6368664375827466, 'xgb__gamma': 0.011637925932845461, 'xgb__reg_lambda': 2.3987502626903376, 'xgb__reg_alpha': 0.4236289205805187, 'xgb__scale_pos_weight': 3.210139138807843}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:21:42,196] Trial 30 finished with value: 0.9554719791472724 and parameters: {'smote__sampling_strategy': 0.8237789660345414, 'smote__k_neighbors': 4, 'xgb__n_estimators': 575, 'xgb__learning_rate': 0.08754108144986092, 'xgb__max_depth': 3, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.5902980912607756, 'xgb__colsample_bytree': 0.7012062390954887, 'xgb__gamma': 0.1295584995398359, 'xgb__reg_lambda': 1.3957498748629764, 'xgb__reg_alpha': 0.5816593579548719, 'xgb__scale_pos_weight': 3.7655508936009916}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:21:54,529] Trial 31 finished with value: 0.9643176317259355 and parameters: {'smote__sampling_strategy': 0.7335966172521562, 'smote__k_neighbors': 3, 'xgb__n_estimators': 467, 'xgb__learning_rate': 0.020633047621508027, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.5004901203245256, 'xgb__colsample_bytree': 0.650126323739448, 'xgb__gamma': 0.17305910402530378, 'xgb__reg_lambda': 3.8722801203326043, 'xgb__reg_alpha': 0.3941824431159283, 'xgb__scale_pos_weight': 4.996686121900996}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:22:06,238] Trial 32 finished with value: 0.9615718674362317 and parameters: {'smote__sampling_strategy': 0.627653046963955, 'smote__k_neighbors': 3, 'xgb__n_estimators': 402, 'xgb__learning_rate': 0.019261738460637914, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.5094363432042746, 'xgb__colsample_bytree': 0.5869644408896452, 'xgb__gamma': 0.058699717725380185, 'xgb__reg_lambda': 3.093090145942625, 'xgb__reg_alpha': 0.3938819721410852, 'xgb__scale_pos_weight': 4.999015741021173}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:22:17,040] Trial 33 finished with value: 0.9609635077266804 and parameters: {'smote__sampling_strategy': 0.502420018957196, 'smote__k_neighbors': 3, 'xgb__n_estimators': 496, 'xgb__learning_rate': 0.01434632080974319, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.5348835554038257, 'xgb__colsample_bytree': 0.638540953779045, 'xgb__gamma': 0.2728330260220323, 'xgb__reg_lambda': 3.6335975724037226, 'xgb__reg_alpha': 0.53106364594177, 'xgb__scale_pos_weight': 4.473822177796549}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:22:28,622] Trial 34 finished with value: 0.9591333085086575 and parameters: {'smote__sampling_strategy': 0.34636635171896124, 'smote__k_neighbors': 3, 'xgb__n_estimators': 683, 'xgb__learning_rate': 0.04678322995528168, 'xgb__max_depth': 4, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.6238469819889708, 'xgb__colsample_bytree': 0.6907458405772804, 'xgb__gamma': 0.23672069112736915, 'xgb__reg_lambda': 1.7202640881822682, 'xgb__reg_alpha': 0.28066486895178366, 'xgb__scale_pos_weight': 4.814644717377299}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:22:41,655] Trial 35 finished with value: 0.9621806926084527 and parameters: {'smote__sampling_strategy': 0.9261613125801751, 'smote__k_neighbors': 3, 'xgb__n_estimators': 442, 'xgb__learning_rate': 0.034793661778207696, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7331487985246538, 'xgb__colsample_bytree': 0.7360135643789198, 'xgb__gamma': 0.17860482677914313, 'xgb__reg_lambda': 4.418715432342766, 'xgb__reg_alpha': 0.7725060862573567, 'xgb__scale_pos_weight': 4.1765728872602566}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:22:57,329] Trial 36 finished with value: 0.9566942841184136 and parameters: {'smote__sampling_strategy': 0.7183900730587601, 'smote__k_neighbors': 4, 'xgb__n_estimators': 613, 'xgb__learning_rate': 0.02107660008571263, 'xgb__max_depth': 6, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.5594277784619391, 'xgb__colsample_bytree': 0.7836622663106565, 'xgb__gamma': 0.3724009442072707, 'xgb__reg_lambda': 2.680724439131689, 'xgb__reg_alpha': 0.1775787448156363, 'xgb__scale_pos_weight': 3.6086086855400197}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:23:08,271] Trial 37 finished with value: 0.9612683857754607 and parameters: {'smote__sampling_strategy': 0.6301898896290388, 'smote__k_neighbors': 3, 'xgb__n_estimators': 353, 'xgb__learning_rate': 0.016726859385201853, 'xgb__max_depth': 7, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.6750384459827946, 'xgb__colsample_bytree': 0.8197956851725665, 'xgb__gamma': 0.08605990141872243, 'xgb__reg_lambda': 0.9241567267085429, 'xgb__reg_alpha': 0.6303481518978523, 'xgb__scale_pos_weight': 4.511430998221181}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:23:22,168] Trial 38 finished with value: 0.9634039285049338 and parameters: {'smote__sampling_strategy': 0.8220827068962212, 'smote__k_neighbors': 3, 'xgb__n_estimators': 560, 'xgb__learning_rate': 0.011723586227269335, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.501653874581102, 'xgb__colsample_bytree': 0.6335633812156836, 'xgb__gamma': 0.4363129208826476, 'xgb__reg_lambda': 2.278162761299409, 'xgb__reg_alpha': 0.48057595554685933, 'xgb__scale_pos_weight': 4.805874169968876}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:23:37,474] Trial 39 finished with value: 0.9448017129026253 and parameters: {'smote__sampling_strategy': 0.9445736241868481, 'smote__k_neighbors': 4, 'xgb__n_estimators': 517, 'xgb__learning_rate': 0.154370987701253, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.614721980896268, 'xgb__colsample_bytree': 0.875541030336609, 'xgb__gamma': 0.05570877190932966, 'xgb__reg_lambda': 0.590524408629755, 'xgb__reg_alpha': 0.966586001703831, 'xgb__scale_pos_weight': 3.4571870450026667}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:23:53,763] Trial 40 finished with value: 0.9615737292869113 and parameters: {'smote__sampling_strategy': 0.8923018410465163, 'smote__k_neighbors': 3, 'xgb__n_estimators': 737, 'xgb__learning_rate': 0.015570316705827475, 'xgb__max_depth': 3, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.7971514993631029, 'xgb__colsample_bytree': 0.7290155236689004, 'xgb__gamma': 0.12367778406012817, 'xgb__reg_lambda': 1.2282120163837498, 'xgb__reg_alpha': 0.5445808469257676, 'xgb__scale_pos_weight': 3.766183861378792}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:24:07,266] Trial 41 finished with value: 0.9643162353379259 and parameters: {'smote__sampling_strategy': 0.7512244909359673, 'smote__k_neighbors': 3, 'xgb__n_estimators': 468, 'xgb__learning_rate': 0.02645458478709883, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8933796097800385, 'xgb__colsample_bytree': 0.6546085569408211, 'xgb__gamma': 0.18897921948301327, 'xgb__reg_lambda': 3.9608189289676115, 'xgb__reg_alpha': 0.07562128645414914, 'xgb__scale_pos_weight': 4.683585487322524}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:24:19,319] Trial 42 finished with value: 0.959438652020108 and parameters: {'smote__sampling_strategy': 0.7313337999350116, 'smote__k_neighbors': 3, 'xgb__n_estimators': 414, 'xgb__learning_rate': 0.027634641458640728, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.9972460731774181, 'xgb__colsample_bytree': 0.6881776521828388, 'xgb__gamma': 0.20160076163009333, 'xgb__reg_lambda': 4.374570672053539, 'xgb__reg_alpha': 0.19266730774379615, 'xgb__scale_pos_weight': 4.518168666462723}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:24:31,191] Trial 43 finished with value: 0.9640108918264755 and parameters: {'smote__sampling_strategy': 0.8474342381800055, 'smote__k_neighbors': 3, 'xgb__n_estimators': 463, 'xgb__learning_rate': 0.035462461622069834, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8854560341897395, 'xgb__colsample_bytree': 0.6015262296521764, 'xgb__gamma': 0.1455472971790313, 'xgb__reg_lambda': 3.664230563556296, 'xgb__reg_alpha': 0.060573390008745565, 'xgb__scale_pos_weight': 4.818050147328701}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:24:39,203] Trial 44 finished with value: 0.9493748836343325 and parameters: {'smote__sampling_strategy': 0.12419230344023852, 'smote__k_neighbors': 3, 'xgb__n_estimators': 490, 'xgb__learning_rate': 0.05296361395983341, 'xgb__max_depth': 5, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.8895818386330667, 'xgb__colsample_bytree': 0.65351509666048, 'xgb__gamma': 0.17870513176248135, 'xgb__reg_lambda': 4.540354838039147, 'xgb__reg_alpha': 0.15284877037074468, 'xgb__scale_pos_weight': 4.096445325918971}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:24:50,721] Trial 45 finished with value: 0.9630967231428039 and parameters: {'smote__sampling_strategy': 0.7863817619780298, 'smote__k_neighbors': 4, 'xgb__n_estimators': 430, 'xgb__learning_rate': 0.04051344289956079, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8272684192027121, 'xgb__colsample_bytree': 0.5682425322125274, 'xgb__gamma': 0.2693057716588019, 'xgb__reg_lambda': 1.653906972687482, 'xgb__reg_alpha': 0.26981851644149857, 'xgb__scale_pos_weight': 4.982070903606589}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:25:02,411] Trial 46 finished with value: 0.9606572332898903 and parameters: {'smote__sampling_strategy': 0.5963972055771378, 'smote__k_neighbors': 3, 'xgb__n_estimators': 461, 'xgb__learning_rate': 0.0222699618967235, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.935930724111083, 'xgb__colsample_bytree': 0.6200791292522693, 'xgb__gamma': 0.22859398792265773, 'xgb__reg_lambda': 3.9638172072222853, 'xgb__reg_alpha': 0.3809262076133267, 'xgb__scale_pos_weight': 4.386936751259634}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:25:11,371] Trial 47 finished with value: 0.9618762800223422 and parameters: {'smote__sampling_strategy': 0.5282903383571105, 'smote__k_neighbors': 5, 'xgb__n_estimators': 375, 'xgb__learning_rate': 0.02767411083211029, 'xgb__max_depth': 4, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.7610548104459365, 'xgb__colsample_bytree': 0.6770464454954436, 'xgb__gamma': 0.3147528598000971, 'xgb__reg_lambda': 3.437268331947351, 'xgb__reg_alpha': 0.6579014403675129, 'xgb__scale_pos_weight': 4.6344887689526155}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:25:24,212] Trial 48 finished with value: 0.9469358592440887 and parameters: {'smote__sampling_strategy': 0.74125351360641, 'smote__k_neighbors': 7, 'xgb__n_estimators': 584, 'xgb__learning_rate': 0.01311520255998188, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.84427959410747, 'xgb__colsample_bytree': 0.70903420916591, 'xgb__gamma': 0.2530474208968423, 'xgb__reg_lambda': 0.8493982636978793, 'xgb__reg_alpha': 0.3345310406115762, 'xgb__scale_pos_weight': 1.6382098495916073}. Best is trial 19 with value: 0.9664531744554086.\n",
      "[I 2025-09-30 16:25:37,837] Trial 49 finished with value: 0.9588284304598771 and parameters: {'smote__sampling_strategy': 0.7900659663933165, 'smote__k_neighbors': 3, 'xgb__n_estimators': 515, 'xgb__learning_rate': 0.025900017744878254, 'xgb__max_depth': 7, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.9595558810514183, 'xgb__colsample_bytree': 0.6530520175683195, 'xgb__gamma': 0.0955797766289077, 'xgb__reg_lambda': 3.0236175812236112, 'xgb__reg_alpha': 0.11108710388482654, 'xgb__scale_pos_weight': 4.220798272402272}. Best is trial 19 with value: 0.9664531744554086.\n",
      "\n",
      "===== Optuna Í≤∞Í≥º =====\n",
      "Best Params: {'smote__sampling_strategy': 0.8720679052176163, 'smote__k_neighbors': 3, 'xgb__n_estimators': 536, 'xgb__learning_rate': 0.011309626065281651, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7817757557752542, 'xgb__colsample_bytree': 0.7610341369684775, 'xgb__gamma': 0.317690601991742, 'xgb__reg_lambda': 1.4536900190258994, 'xgb__reg_alpha': 0.30245181417777384, 'xgb__scale_pos_weight': 4.511464146520451}\n",
      "Best Recall: 0.9664531744554086\n",
      "\n",
      "===== Top 10 Trials (Recall Í∏∞Ï§Ä) =====\n",
      "Rank 1 | Recall=0.9665 | Params={'smote__sampling_strategy': 0.8720679052176163, 'smote__k_neighbors': 3, 'xgb__n_estimators': 536, 'xgb__learning_rate': 0.011309626065281651, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7817757557752542, 'xgb__colsample_bytree': 0.7610341369684775, 'xgb__gamma': 0.317690601991742, 'xgb__reg_lambda': 1.4536900190258994, 'xgb__reg_alpha': 0.30245181417777384, 'xgb__scale_pos_weight': 4.511464146520451}\n",
      "Rank 2 | Recall=0.9643 | Params={'smote__sampling_strategy': 0.7335966172521562, 'smote__k_neighbors': 3, 'xgb__n_estimators': 467, 'xgb__learning_rate': 0.020633047621508027, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.5004901203245256, 'xgb__colsample_bytree': 0.650126323739448, 'xgb__gamma': 0.17305910402530378, 'xgb__reg_lambda': 3.8722801203326043, 'xgb__reg_alpha': 0.3941824431159283, 'xgb__scale_pos_weight': 4.996686121900996}\n",
      "Rank 3 | Recall=0.9643 | Params={'smote__sampling_strategy': 0.9349318927976972, 'smote__k_neighbors': 3, 'xgb__n_estimators': 569, 'xgb__learning_rate': 0.035695536700790745, 'xgb__max_depth': 4, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.6372443836612128, 'xgb__colsample_bytree': 0.6350485140990396, 'xgb__gamma': 0.19737583329506736, 'xgb__reg_lambda': 3.8331170255513443, 'xgb__reg_alpha': 0.5874044485123946, 'xgb__scale_pos_weight': 4.995667617332649}\n",
      "Rank 4 | Recall=0.9643 | Params={'smote__sampling_strategy': 0.7725164093513868, 'smote__k_neighbors': 3, 'xgb__n_estimators': 455, 'xgb__learning_rate': 0.02311736795959936, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8185197377844964, 'xgb__colsample_bytree': 0.6857768506727961, 'xgb__gamma': 0.21701344916614496, 'xgb__reg_lambda': 2.4283127317410798, 'xgb__reg_alpha': 0.06611769764790727, 'xgb__scale_pos_weight': 4.5217303229157935}\n",
      "Rank 5 | Recall=0.9643 | Params={'smote__sampling_strategy': 0.7512244909359673, 'smote__k_neighbors': 3, 'xgb__n_estimators': 468, 'xgb__learning_rate': 0.02645458478709883, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8933796097800385, 'xgb__colsample_bytree': 0.6546085569408211, 'xgb__gamma': 0.18897921948301327, 'xgb__reg_lambda': 3.9608189289676115, 'xgb__reg_alpha': 0.07562128645414914, 'xgb__scale_pos_weight': 4.683585487322524}\n",
      "Rank 6 | Recall=0.9640 | Params={'smote__sampling_strategy': 0.7596745040621485, 'smote__k_neighbors': 3, 'xgb__n_estimators': 467, 'xgb__learning_rate': 0.025484132975590136, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8604854416532041, 'xgb__colsample_bytree': 0.6601468936545319, 'xgb__gamma': 0.1801862854881622, 'xgb__reg_lambda': 3.938181975863292, 'xgb__reg_alpha': 0.009885698950640384, 'xgb__scale_pos_weight': 4.68106127387665}\n",
      "Rank 7 | Recall=0.9640 | Params={'smote__sampling_strategy': 0.8474342381800055, 'smote__k_neighbors': 3, 'xgb__n_estimators': 463, 'xgb__learning_rate': 0.035462461622069834, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8854560341897395, 'xgb__colsample_bytree': 0.6015262296521764, 'xgb__gamma': 0.1455472971790313, 'xgb__reg_lambda': 3.664230563556296, 'xgb__reg_alpha': 0.060573390008745565, 'xgb__scale_pos_weight': 4.818050147328701}\n",
      "Rank 8 | Recall=0.9637 | Params={'smote__sampling_strategy': 0.9669131610162475, 'smote__k_neighbors': 3, 'xgb__n_estimators': 521, 'xgb__learning_rate': 0.03402134900072943, 'xgb__max_depth': 4, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.6343370273674797, 'xgb__colsample_bytree': 0.5799192204707221, 'xgb__gamma': 0.206562774816221, 'xgb__reg_lambda': 4.965013957087807, 'xgb__reg_alpha': 0.7342225126578525, 'xgb__scale_pos_weight': 4.33286884459314}\n",
      "Rank 9 | Recall=0.9634 | Params={'smote__sampling_strategy': 0.8220827068962212, 'smote__k_neighbors': 3, 'xgb__n_estimators': 560, 'xgb__learning_rate': 0.011723586227269335, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.501653874581102, 'xgb__colsample_bytree': 0.6335633812156836, 'xgb__gamma': 0.4363129208826476, 'xgb__reg_lambda': 2.278162761299409, 'xgb__reg_alpha': 0.48057595554685933, 'xgb__scale_pos_weight': 4.805874169968876}\n",
      "Rank 10 | Recall=0.9634 | Params={'smote__sampling_strategy': 0.769486427420692, 'smote__k_neighbors': 3, 'xgb__n_estimators': 447, 'xgb__learning_rate': 0.02328154552808944, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8198855682650426, 'xgb__colsample_bytree': 0.6587533850290036, 'xgb__gamma': 0.2199559892151777, 'xgb__reg_lambda': 2.2120039847125437, 'xgb__reg_alpha': 0.10551744616035731, 'xgb__scale_pos_weight': 4.419781154392707}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import optuna\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# ======================\n",
    "class DatetimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"date\" in df.columns and \"time\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(\n",
    "                df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n",
    "                errors=\"coerce\",\n",
    "                infer_datetime_format=True\n",
    "            )\n",
    "        if \"datetime\" in df.columns:\n",
    "            df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "            df[\"shift\"] = df[\"hour\"].apply(lambda h: \"Day\" if 8 <= h <= 19 else \"Night\")\n",
    "            prev_count = df[\"count\"].iloc[0]\n",
    "            global_counts, accum = [], 0\n",
    "            for current_count in df[\"count\"]:\n",
    "                if current_count < prev_count:\n",
    "                    accum += prev_count\n",
    "                global_counts.append(accum + current_count)\n",
    "                prev_count = current_count\n",
    "            df[\"global_count\"] = global_counts\n",
    "            df[\"year_month\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "            df[\"monthly_count\"] = df.groupby(\"year_month\").cumcount() + 1\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"low_section_speed\" in df.columns and \"high_section_speed\" in df.columns:\n",
    "            df[\"speed_ratio\"] = df[\"low_section_speed\"] / df[\"high_section_speed\"]\n",
    "            df[\"pressure_speed_ratio\"] = df[\"cast_pressure\"] / df[\"high_section_speed\"]\n",
    "            df.loc[(df[\"low_section_speed\"] == 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "            df.loc[(df[\"low_section_speed\"] != 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "            df.loc[df[\"high_section_speed\"] == 0, \"pressure_speed_ratio\"] = -1\n",
    "\n",
    "        for col in [\"heating_furnace\", \"emergency_stop\", \"tryshot_signal\", \"EMS_operation_time\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "        if \"molten_temp\" in df.columns and df[\"molten_temp\"].isna().any():\n",
    "            df[\"molten_temp\"] = df[\"molten_temp\"].fillna(df[\"molten_temp\"].mode()[0])\n",
    "\n",
    "        if \"molten_volume\" in df.columns:\n",
    "            df[\"molten_volume\"] = df[\"molten_volume\"].interpolate(\"linear\").ffill().bfill()\n",
    "\n",
    "        df = df.replace([np.inf, -np.inf], -1)\n",
    "        return df\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols=None):\n",
    "        self.drop_cols = drop_cols or []\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=[c for c in self.drop_cols if c in X.columns])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Threshold Finder\n",
    "# ======================\n",
    "def find_best_threshold_fbeta(y_true, y_prob, beta=2.0):\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob)\n",
    "    t = np.append(t, 1.0)\n",
    "    fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(fbeta))\n",
    "    return float(t[best_idx]), float(fbeta[best_idx])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    test  = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "    y_train = train[\"passorfail\"]\n",
    "    X_train = train.drop(columns=[\"passorfail\"])\n",
    "    X_test  = test.copy()\n",
    "\n",
    "    # ----- ÎìúÎûçÌï† Ïª¨Îüº\n",
    "    drop_cols = [\"id\",\"line\",\"name\",\"mold_name\",\"date\",\"time\",\"registration_time\",\n",
    "                 \"year_month\",\"hour\",\"datetime\",\"real_time\",\"working\"]\n",
    "\n",
    "    # ----- Ïπ¥ÌÖåÍ≥†Î¶¨/ÏàòÏπòÌòï Íµ¨Î∂Ñ\n",
    "    tmp_after = (DatetimeFeatureExtractor().fit_transform(X_train))\n",
    "    tmp_after = (FeatureEngineer().fit_transform(tmp_after))\n",
    "    tmp_after = DropColumns(drop_cols=drop_cols).fit_transform(tmp_after)\n",
    "\n",
    "    expected_cats = [\"mold_code\",\"heating_furnace\",\"EMS_operation_time\",\"shift\",\n",
    "                     \"emergency_stop\",\"tryshot_signal\"]\n",
    "    present_cats = [c for c in expected_cats if c in tmp_after.columns]\n",
    "\n",
    "    cat_pipe = SkPipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ])\n",
    "    num_pipe = SkPipeline(steps=[(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
    "    num_selector = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "    model_preproc = ColumnTransformer(\n",
    "        transformers=[(\"cat\", cat_pipe, present_cats),\n",
    "                      (\"num\", num_pipe, num_selector)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    categorical_feature_indices = list(range(len(present_cats)))\n",
    "\n",
    "    # ----- ÏµúÏ¢Ö ÌååÏù¥ÌîÑÎùºÏù∏\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"datetime\", DatetimeFeatureExtractor()),\n",
    "        (\"engineer\", FeatureEngineer()),\n",
    "        (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "        (\"prep\", model_preproc),   # Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨ Ìè¨Ìï®\n",
    "        (\"smote\", SMOTENC(categorical_features=categorical_feature_indices, random_state=42)),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # ======================\n",
    "    # Optuna Objective\n",
    "    # ======================\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"smote__sampling_strategy\": trial.suggest_float(\"smote__sampling_strategy\", 0.1, 1.0),\n",
    "            \"smote__k_neighbors\": trial.suggest_int(\"smote__k_neighbors\", 3, 7),\n",
    "            \"xgb__n_estimators\": trial.suggest_int(\"xgb__n_estimators\", 300, 800),\n",
    "            \"xgb__learning_rate\": trial.suggest_float(\"xgb__learning_rate\", 0.01, 0.2, log=True),\n",
    "            \"xgb__max_depth\": trial.suggest_int(\"xgb__max_depth\", 3, 8),\n",
    "            \"xgb__min_child_weight\": trial.suggest_int(\"xgb__min_child_weight\", 1, 5),\n",
    "            \"xgb__subsample\": trial.suggest_float(\"xgb__subsample\", 0.5, 1.0),\n",
    "            \"xgb__colsample_bytree\": trial.suggest_float(\"xgb__colsample_bytree\", 0.5, 1.0),\n",
    "            \"xgb__gamma\": trial.suggest_float(\"xgb__gamma\", 0.0, 0.5),\n",
    "            \"xgb__reg_lambda\": trial.suggest_float(\"xgb__reg_lambda\", 0.5, 5.0, log=True),\n",
    "            \"xgb__reg_alpha\": trial.suggest_float(\"xgb__reg_alpha\", 0.0, 1.0),\n",
    "            \"xgb__scale_pos_weight\": trial.suggest_float(\"xgb__scale_pos_weight\", 1.0, 5.0),\n",
    "        }\n",
    "        pipe.set_params(**params)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    # ======================\n",
    "    # Optuna Ïã§Ìñâ\n",
    "    # ======================\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=True)  # trial Ïàò Ï°∞Ï†à Í∞ÄÎä•\n",
    "\n",
    "    # ======================\n",
    "    # Í≤∞Í≥º Ï∂úÎ†•\n",
    "    # ======================\n",
    "    print(\"\\n===== Optuna Í≤∞Í≥º =====\")\n",
    "    print(\"Best Params:\", study.best_trial.params)\n",
    "    print(\"Best Recall:\", study.best_value)\n",
    "\n",
    "    print(\"\\n===== Top 10 Trials (Recall Í∏∞Ï§Ä) =====\")\n",
    "    top_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else -1, reverse=True)[:10]\n",
    "    for i, t in enumerate(top_trials, 1):\n",
    "        print(f\"Rank {i} | Recall={t.value:.4f} | Params={t.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08605ec6-564f-40fc-bced-9d2216a44880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Top 10 Trials (Recall/F1/Acc) =====\n",
      "Rank 1 | Recall=0.9665 | F1=0.8440 | Acc=0.9841 | Params={'smote__sampling_strategy': 0.8720679052176163, 'smote__k_neighbors': 3, 'xgb__n_estimators': 536, 'xgb__learning_rate': 0.011309626065281651, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7817757557752542, 'xgb__colsample_bytree': 0.7610341369684775, 'xgb__gamma': 0.317690601991742, 'xgb__reg_lambda': 1.4536900190258994, 'xgb__reg_alpha': 0.30245181417777384, 'xgb__scale_pos_weight': 4.511464146520451}\n",
      "Rank 2 | Recall=0.9643 | F1=0.8450 | Acc=0.9842 | Params={'smote__sampling_strategy': 0.7335966172521562, 'smote__k_neighbors': 3, 'xgb__n_estimators': 467, 'xgb__learning_rate': 0.020633047621508027, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.5004901203245256, 'xgb__colsample_bytree': 0.650126323739448, 'xgb__gamma': 0.17305910402530378, 'xgb__reg_lambda': 3.8722801203326043, 'xgb__reg_alpha': 0.3941824431159283, 'xgb__scale_pos_weight': 4.996686121900996}\n",
      "Rank 3 | Recall=0.9643 | F1=0.8669 | Acc=0.9868 | Params={'smote__sampling_strategy': 0.9349318927976972, 'smote__k_neighbors': 3, 'xgb__n_estimators': 569, 'xgb__learning_rate': 0.035695536700790745, 'xgb__max_depth': 4, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.6372443836612128, 'xgb__colsample_bytree': 0.6350485140990396, 'xgb__gamma': 0.19737583329506736, 'xgb__reg_lambda': 3.8331170255513443, 'xgb__reg_alpha': 0.5874044485123946, 'xgb__scale_pos_weight': 4.995667617332649}\n",
      "Rank 4 | Recall=0.9643 | F1=0.8524 | Acc=0.9851 | Params={'smote__sampling_strategy': 0.7725164093513868, 'smote__k_neighbors': 3, 'xgb__n_estimators': 455, 'xgb__learning_rate': 0.02311736795959936, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8185197377844964, 'xgb__colsample_bytree': 0.6857768506727961, 'xgb__gamma': 0.21701344916614496, 'xgb__reg_lambda': 2.4283127317410798, 'xgb__reg_alpha': 0.06611769764790727, 'xgb__scale_pos_weight': 4.5217303229157935}\n",
      "Rank 5 | Recall=0.9643 | F1=0.8728 | Acc=0.9875 | Params={'smote__sampling_strategy': 0.7512244909359673, 'smote__k_neighbors': 3, 'xgb__n_estimators': 468, 'xgb__learning_rate': 0.02645458478709883, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8933796097800385, 'xgb__colsample_bytree': 0.6546085569408211, 'xgb__gamma': 0.18897921948301327, 'xgb__reg_lambda': 3.9608189289676115, 'xgb__reg_alpha': 0.07562128645414914, 'xgb__scale_pos_weight': 4.683585487322524}\n",
      "Rank 6 | Recall=0.9640 | F1=0.8701 | Acc=0.9872 | Params={'smote__sampling_strategy': 0.7596745040621485, 'smote__k_neighbors': 3, 'xgb__n_estimators': 467, 'xgb__learning_rate': 0.025484132975590136, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8604854416532041, 'xgb__colsample_bytree': 0.6601468936545319, 'xgb__gamma': 0.1801862854881622, 'xgb__reg_lambda': 3.938181975863292, 'xgb__reg_alpha': 0.009885698950640384, 'xgb__scale_pos_weight': 4.68106127387665}\n",
      "Rank 7 | Recall=0.9640 | F1=0.8500 | Acc=0.9848 | Params={'smote__sampling_strategy': 0.8474342381800055, 'smote__k_neighbors': 3, 'xgb__n_estimators': 463, 'xgb__learning_rate': 0.035462461622069834, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8854560341897395, 'xgb__colsample_bytree': 0.6015262296521764, 'xgb__gamma': 0.1455472971790313, 'xgb__reg_lambda': 3.664230563556296, 'xgb__reg_alpha': 0.060573390008745565, 'xgb__scale_pos_weight': 4.818050147328701}\n",
      "Rank 8 | Recall=0.9637 | F1=0.8621 | Acc=0.9863 | Params={'smote__sampling_strategy': 0.9669131610162475, 'smote__k_neighbors': 3, 'xgb__n_estimators': 521, 'xgb__learning_rate': 0.03402134900072943, 'xgb__max_depth': 4, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.6343370273674797, 'xgb__colsample_bytree': 0.5799192204707221, 'xgb__gamma': 0.206562774816221, 'xgb__reg_lambda': 4.965013957087807, 'xgb__reg_alpha': 0.7342225126578525, 'xgb__scale_pos_weight': 4.33286884459314}\n",
      "Rank 9 | Recall=0.9634 | F1=0.7812 | Acc=0.9759 | Params={'smote__sampling_strategy': 0.8220827068962212, 'smote__k_neighbors': 3, 'xgb__n_estimators': 560, 'xgb__learning_rate': 0.011723586227269335, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.501653874581102, 'xgb__colsample_bytree': 0.6335633812156836, 'xgb__gamma': 0.4363129208826476, 'xgb__reg_lambda': 2.278162761299409, 'xgb__reg_alpha': 0.48057595554685933, 'xgb__scale_pos_weight': 4.805874169968876}\n",
      "Rank 10 | Recall=0.9634 | F1=0.8583 | Acc=0.9858 | Params={'smote__sampling_strategy': 0.769486427420692, 'smote__k_neighbors': 3, 'xgb__n_estimators': 447, 'xgb__learning_rate': 0.02328154552808944, 'xgb__max_depth': 5, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8198855682650426, 'xgb__colsample_bytree': 0.6587533850290036, 'xgb__gamma': 0.2199559892151777, 'xgb__reg_lambda': 2.2120039847125437, 'xgb__reg_alpha': 0.10551744616035731, 'xgb__scale_pos_weight': 4.419781154392707}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Top 10 trialÎì§\n",
    "top_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:10]\n",
    "\n",
    "print(\"\\n===== Top 10 Trials (Recall/F1/Acc) =====\")\n",
    "for rank, t in enumerate(top_trials, 1):\n",
    "    params = t.params\n",
    "    pipe.set_params(**params)\n",
    "    \n",
    "    recall = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1).mean()\n",
    "    f1     = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    acc    = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "    \n",
    "    print(f\"Rank {rank} | Recall={recall:.4f} | F1={f1:.4f} | Acc={acc:.4f} | Params={params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416ca9c-0557-4c89-849d-6401fe75709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-30 17:22:29,205] A new study created in memory with name: no-name-37da6441-fe5a-400b-981e-d49262d9bdb0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c6701cace14ffabd1192c2c9491965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-09-30 17:23:19,088] Trial 0 finished with value: 0.9274846689280454 and parameters: {'smote__sampling_strategy': 0.5123153125572841, 'smote__k_neighbors': 4, 'xgb__n_estimators': 735, 'xgb__learning_rate': 0.2681302634628144, 'xgb__max_depth': 6, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.7080121461970799, 'xgb__colsample_bytree': 0.9503672472420313, 'xgb__gamma': 0.01835207105159231, 'xgb__reg_lambda': 2.4345386568714247, 'xgb__reg_alpha': 0.9748765493561431, 'xgb__scale_pos_weight': 2.659892366437899}. Best is trial 0 with value: 0.9274846689280454.\n",
      "[I 2025-09-30 17:23:49,632] Trial 1 finished with value: 0.9189639837763176 and parameters: {'smote__sampling_strategy': 0.9796673696001282, 'smote__k_neighbors': 7, 'xgb__n_estimators': 233, 'xgb__learning_rate': 0.22064148407980677, 'xgb__max_depth': 4, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.8450044250447621, 'xgb__colsample_bytree': 0.9015613476941459, 'xgb__gamma': 0.28553033056082033, 'xgb__reg_lambda': 1.1290974708614765, 'xgb__reg_alpha': 0.5239672286281172, 'xgb__scale_pos_weight': 2.1500520415590723}. Best is trial 0 with value: 0.9274846689280454.\n",
      "[I 2025-09-30 17:24:10,419] Trial 2 finished with value: 0.9227108793672951 and parameters: {'smote__sampling_strategy': 0.36198581073238245, 'smote__k_neighbors': 7, 'xgb__n_estimators': 224, 'xgb__learning_rate': 0.23751683381490124, 'xgb__max_depth': 5, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.7480113777857645, 'xgb__colsample_bytree': 0.8827039503388208, 'xgb__gamma': 0.47593995960132685, 'xgb__reg_lambda': 3.6913379210200388, 'xgb__reg_alpha': 0.533367219229035, 'xgb__scale_pos_weight': 1.2984734190789546}. Best is trial 0 with value: 0.9274846689280454.\n",
      "[I 2025-09-30 17:24:29,574] Trial 3 finished with value: 0.929602959778391 and parameters: {'smote__sampling_strategy': 0.24277328240957533, 'smote__k_neighbors': 7, 'xgb__n_estimators': 205, 'xgb__learning_rate': 0.03564860506900945, 'xgb__max_depth': 6, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.6589076079479458, 'xgb__colsample_bytree': 0.6659387525315837, 'xgb__gamma': 0.1461433784978569, 'xgb__reg_lambda': 2.794176304104659, 'xgb__reg_alpha': 0.7700553676471812, 'xgb__scale_pos_weight': 1.5288461296972518}. Best is trial 3 with value: 0.929602959778391.\n",
      "[I 2025-09-30 17:24:48,446] Trial 4 finished with value: 0.9228405148257899 and parameters: {'smote__sampling_strategy': 0.32297895970525425, 'smote__k_neighbors': 5, 'xgb__n_estimators': 210, 'xgb__learning_rate': 0.22559733771314863, 'xgb__max_depth': 4, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7101847971161395, 'xgb__colsample_bytree': 0.9120349397247438, 'xgb__gamma': 0.16931482142483212, 'xgb__reg_lambda': 3.976985007317535, 'xgb__reg_alpha': 0.8469138319277262, 'xgb__scale_pos_weight': 1.277911942928736}. Best is trial 3 with value: 0.929602959778391.\n",
      "[I 2025-09-30 17:25:18,310] Trial 5 finished with value: 0.9346030195970512 and parameters: {'smote__sampling_strategy': 0.558059461142102, 'smote__k_neighbors': 4, 'xgb__n_estimators': 371, 'xgb__learning_rate': 0.183428590734382, 'xgb__max_depth': 7, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.7765208731135336, 'xgb__colsample_bytree': 0.7785054848584276, 'xgb__gamma': 0.12641770702000887, 'xgb__reg_lambda': 3.557081305858391, 'xgb__reg_alpha': 0.42820926423682937, 'xgb__scale_pos_weight': 2.168421453124834}. Best is trial 5 with value: 0.9346030195970512.\n",
      "[I 2025-09-30 17:26:03,526] Trial 6 finished with value: 0.9289810319880949 and parameters: {'smote__sampling_strategy': 0.9544551510608738, 'smote__k_neighbors': 5, 'xgb__n_estimators': 782, 'xgb__learning_rate': 0.20280508055866675, 'xgb__max_depth': 7, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.813188740624111, 'xgb__colsample_bytree': 0.8702118281019946, 'xgb__gamma': 0.35173139700829886, 'xgb__reg_lambda': 3.5852398385233517, 'xgb__reg_alpha': 0.5412684764981266, 'xgb__scale_pos_weight': 1.7253681190687913}. Best is trial 5 with value: 0.9346030195970512.\n",
      "[I 2025-09-30 17:26:38,473] Trial 7 finished with value: 0.9390046030508419 and parameters: {'smote__sampling_strategy': 0.4549305146123501, 'smote__k_neighbors': 3, 'xgb__n_estimators': 572, 'xgb__learning_rate': 0.06556240865619667, 'xgb__max_depth': 7, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.8687565435277977, 'xgb__colsample_bytree': 0.7901990832052758, 'xgb__gamma': 0.15423635817942682, 'xgb__reg_lambda': 1.59131503728321, 'xgb__reg_alpha': 0.13355972187250842, 'xgb__scale_pos_weight': 3.120634591280754}. Best is trial 7 with value: 0.9390046030508419.\n",
      "[I 2025-09-30 17:27:14,759] Trial 8 finished with value: 0.9453591743004427 and parameters: {'smote__sampling_strategy': 0.2647853416814887, 'smote__k_neighbors': 4, 'xgb__n_estimators': 628, 'xgb__learning_rate': 0.05086252000356245, 'xgb__max_depth': 8, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.8530094930060577, 'xgb__colsample_bytree': 0.6182800176898097, 'xgb__gamma': 0.1750937208548009, 'xgb__reg_lambda': 2.4327841540573987, 'xgb__reg_alpha': 0.4389356144876564, 'xgb__scale_pos_weight': 3.212537709007618}. Best is trial 8 with value: 0.9453591743004427.\n",
      "[I 2025-09-30 17:27:52,655] Trial 9 finished with value: 0.9209936259743715 and parameters: {'smote__sampling_strategy': 0.5387691086410865, 'smote__k_neighbors': 6, 'xgb__n_estimators': 651, 'xgb__learning_rate': 0.021325954442176644, 'xgb__max_depth': 6, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.9757339112112058, 'xgb__colsample_bytree': 0.866445476410602, 'xgb__gamma': 0.2375981346198487, 'xgb__reg_lambda': 1.7745473937139318, 'xgb__reg_alpha': 0.9676466455485467, 'xgb__scale_pos_weight': 1.8300070578012333}. Best is trial 8 with value: 0.9453591743004427.\n",
      "[I 2025-09-30 17:28:18,065] Trial 10 finished with value: 0.9424470124558093 and parameters: {'smote__sampling_strategy': 0.11965501932923872, 'smote__k_neighbors': 3, 'xgb__n_estimators': 457, 'xgb__learning_rate': 0.11715200132512371, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.9332122389993538, 'xgb__colsample_bytree': 0.6069084042685738, 'xgb__gamma': 0.005687187543013006, 'xgb__reg_lambda': 0.058137175294905585, 'xgb__reg_alpha': 0.015149835243039145, 'xgb__scale_pos_weight': 4.274488833494143}. Best is trial 8 with value: 0.9453591743004427.\n",
      "[I 2025-09-30 17:28:44,213] Trial 11 finished with value: 0.945927570520124 and parameters: {'smote__sampling_strategy': 0.16346631910072434, 'smote__k_neighbors': 3, 'xgb__n_estimators': 460, 'xgb__learning_rate': 0.08935840139455938, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.9431453956923186, 'xgb__colsample_bytree': 0.6062290898399862, 'xgb__gamma': 0.00815174533087204, 'xgb__reg_lambda': 0.3830679965161744, 'xgb__reg_alpha': 0.08632383505701345, 'xgb__scale_pos_weight': 4.1944855263118965}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:29:10,410] Trial 12 finished with value: 0.9449411312597814 and parameters: {'smote__sampling_strategy': 0.11957923916825053, 'smote__k_neighbors': 4, 'xgb__n_estimators': 505, 'xgb__learning_rate': 0.09787851387885664, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.9018313746199242, 'xgb__colsample_bytree': 0.7032072378265427, 'xgb__gamma': 0.07430844605254956, 'xgb__reg_lambda': 0.4363564683238788, 'xgb__reg_alpha': 0.27793780549896613, 'xgb__scale_pos_weight': 4.157449425098291}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:29:41,236] Trial 13 finished with value: 0.9377540332340363 and parameters: {'smote__sampling_strategy': 0.6949594064519208, 'smote__k_neighbors': 3, 'xgb__n_estimators': 383, 'xgb__learning_rate': 0.13192287834299976, 'xgb__max_depth': 8, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.9907877337818866, 'xgb__colsample_bytree': 0.6079210330117328, 'xgb__gamma': 0.23898083768482725, 'xgb__reg_lambda': 4.478534952051046, 'xgb__reg_alpha': 0.2829861768961772, 'xgb__scale_pos_weight': 4.911135193859344}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:30:07,217] Trial 14 finished with value: 0.9192641589599775 and parameters: {'smote__sampling_strategy': 0.22763429964697723, 'smote__k_neighbors': 4, 'xgb__n_estimators': 633, 'xgb__learning_rate': 0.07756618628604096, 'xgb__max_depth': 3, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.9300778015190506, 'xgb__colsample_bytree': 0.6970134987378179, 'xgb__gamma': 0.366365145269122, 'xgb__reg_lambda': 0.9147851824917066, 'xgb__reg_alpha': 0.012294991541125622, 'xgb__scale_pos_weight': 3.478108884147347}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:30:44,441] Trial 15 finished with value: 0.9360359506159719 and parameters: {'smote__sampling_strategy': 0.6946386448767351, 'smote__k_neighbors': 3, 'xgb__n_estimators': 509, 'xgb__learning_rate': 0.14530977580370588, 'xgb__max_depth': 8, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.871894749931413, 'xgb__colsample_bytree': 0.6647097377007741, 'xgb__gamma': 0.07852776705934446, 'xgb__reg_lambda': 2.692993234886963, 'xgb__reg_alpha': 0.692494276806267, 'xgb__scale_pos_weight': 3.822312020907294}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:31:08,900] Trial 16 finished with value: 0.9315883219229806 and parameters: {'smote__sampling_strategy': 0.24580465267270757, 'smote__k_neighbors': 4, 'xgb__n_estimators': 366, 'xgb__learning_rate': 0.05156616321404055, 'xgb__max_depth': 7, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8197467410074264, 'xgb__colsample_bytree': 0.7382382035152645, 'xgb__gamma': 0.0731108313328391, 'xgb__reg_lambda': 2.1315326973161786, 'xgb__reg_alpha': 0.28973826235375394, 'xgb__scale_pos_weight': 4.838969200957534}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:31:40,455] Trial 17 finished with value: 0.8989127774703448 and parameters: {'smote__sampling_strategy': 0.386371282603271, 'smote__k_neighbors': 5, 'xgb__n_estimators': 610, 'xgb__learning_rate': 0.013813157755473121, 'xgb__max_depth': 5, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.9461685958983193, 'xgb__colsample_bytree': 0.6494544465561136, 'xgb__gamma': 0.31584866192001737, 'xgb__reg_lambda': 4.9493417480767015, 'xgb__reg_alpha': 0.14569261476573603, 'xgb__scale_pos_weight': 2.8663342484843}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:32:10,977] Trial 18 finished with value: 0.9451916481831913 and parameters: {'smote__sampling_strategy': 0.12035733123536313, 'smote__k_neighbors': 6, 'xgb__n_estimators': 708, 'xgb__learning_rate': 0.100375236661589, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8805086460990815, 'xgb__colsample_bytree': 0.6032108759975742, 'xgb__gamma': 0.19748926599388644, 'xgb__reg_lambda': 0.9833784930264604, 'xgb__reg_alpha': 0.41362069103775717, 'xgb__scale_pos_weight': 3.4318480614917295}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:32:45,783] Trial 19 finished with value: 0.9300917478419375 and parameters: {'smote__sampling_strategy': 0.6491497256744208, 'smote__k_neighbors': 3, 'xgb__n_estimators': 557, 'xgb__learning_rate': 0.1758412408543394, 'xgb__max_depth': 7, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.9082062888518523, 'xgb__colsample_bytree': 0.7311063025194229, 'xgb__gamma': 0.4142704067825056, 'xgb__reg_lambda': 3.1514564634928313, 'xgb__reg_alpha': 0.6421316273637748, 'xgb__scale_pos_weight': 4.3877088577703445}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:33:27,473] Trial 20 finished with value: 0.9372291680966033 and parameters: {'smote__sampling_strategy': 0.846060702075978, 'smote__k_neighbors': 4, 'xgb__n_estimators': 435, 'xgb__learning_rate': 0.09125901493420394, 'xgb__max_depth': 8, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.6205913592129274, 'xgb__colsample_bytree': 0.6494626938442696, 'xgb__gamma': 0.05318229466451886, 'xgb__reg_lambda': 1.586950163165794, 'xgb__reg_alpha': 0.1764492473144631, 'xgb__scale_pos_weight': 3.806038971898074}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:33:56,642] Trial 21 finished with value: 0.9445783109663853 and parameters: {'smote__sampling_strategy': 0.16302865631007024, 'smote__k_neighbors': 6, 'xgb__n_estimators': 675, 'xgb__learning_rate': 0.11019777407984895, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8751331697953804, 'xgb__colsample_bytree': 0.6141143801148696, 'xgb__gamma': 0.19859275725297967, 'xgb__reg_lambda': 0.7491717064284391, 'xgb__reg_alpha': 0.42506753422232135, 'xgb__scale_pos_weight': 3.2945364391239647}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:34:29,486] Trial 22 finished with value: 0.9416251553314584 and parameters: {'smote__sampling_strategy': 0.19891732611060192, 'smote__k_neighbors': 6, 'xgb__n_estimators': 695, 'xgb__learning_rate': 0.04888377429215108, 'xgb__max_depth': 7, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8372104491401344, 'xgb__colsample_bytree': 0.6264860747759224, 'xgb__gamma': 0.2044283742383714, 'xgb__reg_lambda': 0.3280808146096007, 'xgb__reg_alpha': 0.38963861618856244, 'xgb__scale_pos_weight': 3.6741317399717826}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:35:09,419] Trial 23 finished with value: 0.9390793202442991 and parameters: {'smote__sampling_strategy': 0.3025850218617287, 'smote__k_neighbors': 6, 'xgb__n_estimators': 780, 'xgb__learning_rate': 0.07301913383322906, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.7823274276722472, 'xgb__colsample_bytree': 0.6841157435349502, 'xgb__gamma': 0.11550721143817451, 'xgb__reg_lambda': 0.9288699137885196, 'xgb__reg_alpha': 0.3553811134453052, 'xgb__scale_pos_weight': 2.5540292702810037}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:35:34,335] Trial 24 finished with value: 0.9396112804379243 and parameters: {'smote__sampling_strategy': 0.11126993543271932, 'smote__k_neighbors': 5, 'xgb__n_estimators': 720, 'xgb__learning_rate': 0.1599373363956848, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.9575819663546368, 'xgb__colsample_bytree': 0.7378897633525514, 'xgb__gamma': 0.2953485552302785, 'xgb__reg_lambda': 1.2627654479785704, 'xgb__reg_alpha': 0.6144467377372618, 'xgb__scale_pos_weight': 4.011044544783216}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:36:03,498] Trial 25 finished with value: 0.9389452102766995 and parameters: {'smote__sampling_strategy': 0.2810053042046605, 'smote__k_neighbors': 3, 'xgb__n_estimators': 572, 'xgb__learning_rate': 0.12743162170441186, 'xgb__max_depth': 7, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8861054665730683, 'xgb__colsample_bytree': 0.8226435450579257, 'xgb__gamma': 0.19422710585427805, 'xgb__reg_lambda': 2.106395302644098, 'xgb__reg_alpha': 0.2112855132965611, 'xgb__scale_pos_weight': 3.3961490601531343}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:36:30,790] Trial 26 finished with value: 0.93745382094174 and parameters: {'smote__sampling_strategy': 0.3944199359686875, 'smote__k_neighbors': 6, 'xgb__n_estimators': 428, 'xgb__learning_rate': 0.0896263922694516, 'xgb__max_depth': 6, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.9164656010338991, 'xgb__colsample_bytree': 0.6360666135060159, 'xgb__gamma': 0.12172893776824904, 'xgb__reg_lambda': 0.13086608119865295, 'xgb__reg_alpha': 0.4509076782556829, 'xgb__scale_pos_weight': 4.552356387612057}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:36:54,168] Trial 27 finished with value: 0.9415337279940067 and parameters: {'smote__sampling_strategy': 0.19555394783222196, 'smote__k_neighbors': 4, 'xgb__n_estimators': 323, 'xgb__learning_rate': 0.05243610255387116, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8416877247599881, 'xgb__colsample_bytree': 0.6020011560106234, 'xgb__gamma': 0.2627066554781623, 'xgb__reg_lambda': 0.526224934436242, 'xgb__reg_alpha': 0.10048803859672753, 'xgb__scale_pos_weight': 2.9640078566448906}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:37:25,693] Trial 28 finished with value: 0.9405182223110199 and parameters: {'smote__sampling_strategy': 0.17682895833405438, 'smote__k_neighbors': 5, 'xgb__n_estimators': 606, 'xgb__learning_rate': 0.03436813474968128, 'xgb__max_depth': 7, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.9611859834265266, 'xgb__colsample_bytree': 0.7122300965084485, 'xgb__gamma': 0.024206601751671997, 'xgb__reg_lambda': 1.24234886463551, 'xgb__reg_alpha': 0.6002220314596488, 'xgb__scale_pos_weight': 3.5114510551001605}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:38:04,426] Trial 29 finished with value: 0.9324551283480291 and parameters: {'smote__sampling_strategy': 0.46265363764666323, 'smote__k_neighbors': 4, 'xgb__n_estimators': 728, 'xgb__learning_rate': 0.29567917502928265, 'xgb__max_depth': 6, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.7380152546684856, 'xgb__colsample_bytree': 0.6745914254833774, 'xgb__gamma': 0.10084130230485859, 'xgb__reg_lambda': 2.4023000446485465, 'xgb__reg_alpha': 0.34708617151730403, 'xgb__scale_pos_weight': 2.5443904558147783}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:38:36,467] Trial 30 finished with value: 0.9355885159610656 and parameters: {'smote__sampling_strategy': 0.3276663514492844, 'smote__k_neighbors': 3, 'xgb__n_estimators': 477, 'xgb__learning_rate': 0.11142567537501036, 'xgb__max_depth': 8, 'xgb__min_child_weight': 3, 'xgb__subsample': 0.9958344739242294, 'xgb__colsample_bytree': 0.9788015822266242, 'xgb__gamma': 0.0426614526128469, 'xgb__reg_lambda': 1.9271388013133595, 'xgb__reg_alpha': 0.24905659957985143, 'xgb__scale_pos_weight': 4.547928669338748}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:39:02,770] Trial 31 finished with value: 0.942960214918078 and parameters: {'smote__sampling_strategy': 0.10284161210844245, 'smote__k_neighbors': 4, 'xgb__n_estimators': 525, 'xgb__learning_rate': 0.10066625419607746, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.895079964474876, 'xgb__colsample_bytree': 0.6370342520385587, 'xgb__gamma': 0.0035021335886248756, 'xgb__reg_lambda': 0.5605493816118909, 'xgb__reg_alpha': 0.3227421954555426, 'xgb__scale_pos_weight': 4.031176311778868}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:39:24,401] Trial 32 finished with value: 0.9447377536354692 and parameters: {'smote__sampling_strategy': 0.1519395847700589, 'smote__k_neighbors': 4, 'xgb__n_estimators': 311, 'xgb__learning_rate': 0.08376632770582448, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8521305239708633, 'xgb__colsample_bytree': 0.7105937980465403, 'xgb__gamma': 0.07582422064647112, 'xgb__reg_lambda': 0.4642275611966279, 'xgb__reg_alpha': 0.08323413805750812, 'xgb__scale_pos_weight': 4.132514598002562}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:39:52,836] Trial 33 finished with value: 0.941812730705632 and parameters: {'smote__sampling_strategy': 0.26073219855689195, 'smote__k_neighbors': 7, 'xgb__n_estimators': 519, 'xgb__learning_rate': 0.06595344793535775, 'xgb__max_depth': 7, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.9066183829907951, 'xgb__colsample_bytree': 0.6397144769541592, 'xgb__gamma': 0.043179242524186784, 'xgb__reg_lambda': 0.03380140043932095, 'xgb__reg_alpha': 0.48506483217734037, 'xgb__scale_pos_weight': 3.184286695472019}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:40:22,043] Trial 34 finished with value: 0.9442299233375782 and parameters: {'smote__sampling_strategy': 0.1506662415897263, 'smote__k_neighbors': 5, 'xgb__n_estimators': 749, 'xgb__learning_rate': 0.1466044022919137, 'xgb__max_depth': 8, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8596388352295404, 'xgb__colsample_bytree': 0.6003412408537206, 'xgb__gamma': 0.17738524290110003, 'xgb__reg_lambda': 0.8793400855645954, 'xgb__reg_alpha': 0.24432824229826894, 'xgb__scale_pos_weight': 3.622805987134144}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:40:45,375] Trial 35 finished with value: 0.9147971415017772 and parameters: {'smote__sampling_strategy': 0.2046702264256789, 'smote__k_neighbors': 5, 'xgb__n_estimators': 548, 'xgb__learning_rate': 0.09951052629677452, 'xgb__max_depth': 3, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.9277796570190279, 'xgb__colsample_bytree': 0.6630197785375788, 'xgb__gamma': 0.22515257759343313, 'xgb__reg_lambda': 1.1568292859854268, 'xgb__reg_alpha': 0.5077728408031276, 'xgb__scale_pos_weight': 4.5823198961590474}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:41:11,150] Trial 36 finished with value: 0.9274915896228805 and parameters: {'smote__sampling_strategy': 0.10612453107885678, 'smote__k_neighbors': 3, 'xgb__n_estimators': 687, 'xgb__learning_rate': 0.03194663717664582, 'xgb__max_depth': 4, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8912938066582876, 'xgb__colsample_bytree': 0.7667589108085145, 'xgb__gamma': 0.09765410839013669, 'xgb__reg_lambda': 1.455428657831906, 'xgb__reg_alpha': 0.3922763855381896, 'xgb__scale_pos_weight': 2.784504641628631}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:41:43,979] Trial 37 finished with value: 0.9334190192754377 and parameters: {'smote__sampling_strategy': 0.3532258225734377, 'smote__k_neighbors': 7, 'xgb__n_estimators': 595, 'xgb__learning_rate': 0.13358732906159568, 'xgb__max_depth': 6, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7887287109674036, 'xgb__colsample_bytree': 0.8208231012067957, 'xgb__gamma': 0.14723380871267217, 'xgb__reg_lambda': 0.6614568512036465, 'xgb__reg_alpha': 0.5550675929827006, 'xgb__scale_pos_weight': 3.8750154189597183}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:42:10,046] Trial 38 finished with value: 0.928847152711362 and parameters: {'smote__sampling_strategy': 0.24797981430726387, 'smote__k_neighbors': 5, 'xgb__n_estimators': 484, 'xgb__learning_rate': 0.058296943974738585, 'xgb__max_depth': 5, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8230138838735517, 'xgb__colsample_bytree': 0.6945120463681468, 'xgb__gamma': 0.13828173195829171, 'xgb__reg_lambda': 2.9587138280330025, 'xgb__reg_alpha': 0.062144241893020424, 'xgb__scale_pos_weight': 2.273545468368111}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:42:46,153] Trial 39 finished with value: 0.939488677794124 and parameters: {'smote__sampling_strategy': 0.42284732614255316, 'smote__k_neighbors': 4, 'xgb__n_estimators': 653, 'xgb__learning_rate': 0.16869209213516, 'xgb__max_depth': 7, 'xgb__min_child_weight': 4, 'xgb__subsample': 0.7596958224640534, 'xgb__colsample_bytree': 0.6262493775212361, 'xgb__gamma': 0.16813102483683484, 'xgb__reg_lambda': 0.23209422182858908, 'xgb__reg_alpha': 0.18961920859379253, 'xgb__scale_pos_weight': 4.153015603291677}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:43:07,328] Trial 40 finished with value: 0.9426803986836928 and parameters: {'smote__sampling_strategy': 0.15177781077559294, 'smote__k_neighbors': 3, 'xgb__n_estimators': 257, 'xgb__learning_rate': 0.07810490780002509, 'xgb__max_depth': 7, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.9690386452850898, 'xgb__colsample_bytree': 0.7563749096362815, 'xgb__gamma': 0.05766192954395212, 'xgb__reg_lambda': 3.285509530406217, 'xgb__reg_alpha': 0.7203552143000452, 'xgb__scale_pos_weight': 3.1691113697155093}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:43:30,603] Trial 41 finished with value: 0.9414058863189736 and parameters: {'smote__sampling_strategy': 0.15132138286251917, 'smote__k_neighbors': 4, 'xgb__n_estimators': 306, 'xgb__learning_rate': 0.08256415842067451, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.852437517933909, 'xgb__colsample_bytree': 0.7176572584196469, 'xgb__gamma': 0.08313586567577289, 'xgb__reg_lambda': 0.4815145508804264, 'xgb__reg_alpha': 0.07527877829091122, 'xgb__scale_pos_weight': 4.331925324787671}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:43:54,403] Trial 42 finished with value: 0.9384593538183983 and parameters: {'smote__sampling_strategy': 0.2899026718568638, 'smote__k_neighbors': 4, 'xgb__n_estimators': 284, 'xgb__learning_rate': 0.19282526381083298, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8059474069051276, 'xgb__colsample_bytree': 0.6649436081578007, 'xgb__gamma': 0.038094596087338804, 'xgb__reg_lambda': 0.40843092158255845, 'xgb__reg_alpha': 0.10994531728309895, 'xgb__scale_pos_weight': 4.100688850212535}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:44:19,667] Trial 43 finished with value: 0.939176665551452 and parameters: {'smote__sampling_strategy': 0.21610402254084596, 'smote__k_neighbors': 4, 'xgb__n_estimators': 399, 'xgb__learning_rate': 0.10133784369474533, 'xgb__max_depth': 8, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.833323575182158, 'xgb__colsample_bytree': 0.8085102410827667, 'xgb__gamma': 0.02095837759379822, 'xgb__reg_lambda': 1.0733783621694206, 'xgb__reg_alpha': 0.04208408544221317, 'xgb__scale_pos_weight': 4.723132413393041}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:44:43,088] Trial 44 finished with value: 0.9422827300441299 and parameters: {'smote__sampling_strategy': 0.15592176085273643, 'smote__k_neighbors': 5, 'xgb__n_estimators': 359, 'xgb__learning_rate': 0.04190442992530613, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8777887995004373, 'xgb__colsample_bytree': 0.8430222220318899, 'xgb__gamma': 0.4807308195661459, 'xgb__reg_lambda': 0.32159165566192993, 'xgb__reg_alpha': 0.1468706428226506, 'xgb__scale_pos_weight': 3.681731572661508}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:45:07,314] Trial 45 finished with value: 0.9388280908575712 and parameters: {'smote__sampling_strategy': 0.2263402185660417, 'smote__k_neighbors': 3, 'xgb__n_estimators': 414, 'xgb__learning_rate': 0.11727940732464975, 'xgb__max_depth': 8, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.9414458909300938, 'xgb__colsample_bytree': 0.9216489553469545, 'xgb__gamma': 0.26033342914710705, 'xgb__reg_lambda': 0.6760106224295305, 'xgb__reg_alpha': 0.29615210560038296, 'xgb__scale_pos_weight': 4.279651518109997}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:45:28,549] Trial 46 finished with value: 0.9452383788750455 and parameters: {'smote__sampling_strategy': 0.1284943987204762, 'smote__k_neighbors': 4, 'xgb__n_estimators': 331, 'xgb__learning_rate': 0.070314810049945, 'xgb__max_depth': 7, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8601339796539915, 'xgb__colsample_bytree': 0.6222087978896433, 'xgb__gamma': 0.09925811201549245, 'xgb__reg_lambda': 1.356828731141754, 'xgb__reg_alpha': 0.2255163401252792, 'xgb__scale_pos_weight': 3.843728646907586}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:45:52,461] Trial 47 finished with value: 0.9373875062039645 and parameters: {'smote__sampling_strategy': 0.10324666574640995, 'smote__k_neighbors': 4, 'xgb__n_estimators': 458, 'xgb__learning_rate': 0.020226694500806636, 'xgb__max_depth': 7, 'xgb__min_child_weight': 2, 'xgb__subsample': 0.9162613538127348, 'xgb__colsample_bytree': 0.619652520418356, 'xgb__gamma': 0.10571082391384684, 'xgb__reg_lambda': 3.8906965198884373, 'xgb__reg_alpha': 0.2236911598692996, 'xgb__scale_pos_weight': 3.378952469510754}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:46:40,327] Trial 48 finished with value: 0.935872253955228 and parameters: {'smote__sampling_strategy': 0.5968404205487426, 'smote__k_neighbors': 6, 'xgb__n_estimators': 759, 'xgb__learning_rate': 0.0670437123751812, 'xgb__max_depth': 7, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8997631255381022, 'xgb__colsample_bytree': 0.6438537615552333, 'xgb__gamma': 0.16350388112823783, 'xgb__reg_lambda': 1.4617398685640342, 'xgb__reg_alpha': 0.4443018334423051, 'xgb__scale_pos_weight': 3.9186822305585327}. Best is trial 11 with value: 0.945927570520124.\n",
      "[I 2025-09-30 17:47:13,338] Trial 49 finished with value: 0.9323834680919575 and parameters: {'smote__sampling_strategy': 0.7781232980974222, 'smote__k_neighbors': 3, 'xgb__n_estimators': 340, 'xgb__learning_rate': 0.21784396993398386, 'xgb__max_depth': 8, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.7082616101620535, 'xgb__colsample_bytree': 0.6547483030251734, 'xgb__gamma': 0.13510221082532006, 'xgb__reg_lambda': 2.4856790761368437, 'xgb__reg_alpha': 0.2730437466261473, 'xgb__scale_pos_weight': 3.0425309471481494}. Best is trial 11 with value: 0.945927570520124.\n",
      "\n",
      "===== Top 10 Trials (F1 Í∏∞Ï§Ä, Recall/Acc Ìè¨Ìï®) =====\n",
      "Rank 1 | F1=0.9459 | Recall=0.9475 | Acc=0.9952 | Params={'smote__sampling_strategy': 0.16346631910072434, 'smote__k_neighbors': 3, 'xgb__n_estimators': 460, 'xgb__learning_rate': 0.08935840139455938, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.9431453956923186, 'xgb__colsample_bytree': 0.6062290898399862, 'xgb__gamma': 0.00815174533087204, 'xgb__reg_lambda': 0.3830679965161744, 'xgb__reg_alpha': 0.08632383505701345, 'xgb__scale_pos_weight': 4.1944855263118965}\n",
      "Rank 2 | F1=0.9454 | Recall=0.9503 | Acc=0.9951 | Params={'smote__sampling_strategy': 0.2647853416814887, 'smote__k_neighbors': 4, 'xgb__n_estimators': 628, 'xgb__learning_rate': 0.05086252000356245, 'xgb__max_depth': 8, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.8530094930060577, 'xgb__colsample_bytree': 0.6182800176898097, 'xgb__gamma': 0.1750937208548009, 'xgb__reg_lambda': 2.4327841540573987, 'xgb__reg_alpha': 0.4389356144876564, 'xgb__scale_pos_weight': 3.212537709007618}\n",
      "Rank 3 | F1=0.9452 | Recall=0.9479 | Acc=0.9951 | Params={'smote__sampling_strategy': 0.1284943987204762, 'smote__k_neighbors': 4, 'xgb__n_estimators': 331, 'xgb__learning_rate': 0.070314810049945, 'xgb__max_depth': 7, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8601339796539915, 'xgb__colsample_bytree': 0.6222087978896433, 'xgb__gamma': 0.09925811201549245, 'xgb__reg_lambda': 1.356828731141754, 'xgb__reg_alpha': 0.2255163401252792, 'xgb__scale_pos_weight': 3.843728646907586}\n",
      "Rank 4 | F1=0.9452 | Recall=0.9418 | Acc=0.9951 | Params={'smote__sampling_strategy': 0.12035733123536313, 'smote__k_neighbors': 6, 'xgb__n_estimators': 708, 'xgb__learning_rate': 0.100375236661589, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8805086460990815, 'xgb__colsample_bytree': 0.6032108759975742, 'xgb__gamma': 0.19748926599388644, 'xgb__reg_lambda': 0.9833784930264604, 'xgb__reg_alpha': 0.41362069103775717, 'xgb__scale_pos_weight': 3.4318480614917295}\n",
      "Rank 5 | F1=0.9449 | Recall=0.9424 | Acc=0.9951 | Params={'smote__sampling_strategy': 0.11957923916825053, 'smote__k_neighbors': 4, 'xgb__n_estimators': 505, 'xgb__learning_rate': 0.09787851387885664, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.9018313746199242, 'xgb__colsample_bytree': 0.7032072378265427, 'xgb__gamma': 0.07430844605254956, 'xgb__reg_lambda': 0.4363564683238788, 'xgb__reg_alpha': 0.27793780549896613, 'xgb__scale_pos_weight': 4.157449425098291}\n",
      "Rank 6 | F1=0.9447 | Recall=0.9494 | Acc=0.9951 | Params={'smote__sampling_strategy': 0.1519395847700589, 'smote__k_neighbors': 4, 'xgb__n_estimators': 311, 'xgb__learning_rate': 0.08376632770582448, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8521305239708633, 'xgb__colsample_bytree': 0.7105937980465403, 'xgb__gamma': 0.07582422064647112, 'xgb__reg_lambda': 0.4642275611966279, 'xgb__reg_alpha': 0.08323413805750812, 'xgb__scale_pos_weight': 4.132514598002562}\n",
      "Rank 7 | F1=0.9446 | Recall=0.9414 | Acc=0.9951 | Params={'smote__sampling_strategy': 0.16302865631007024, 'smote__k_neighbors': 6, 'xgb__n_estimators': 675, 'xgb__learning_rate': 0.11019777407984895, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.8751331697953804, 'xgb__colsample_bytree': 0.6141143801148696, 'xgb__gamma': 0.19859275725297967, 'xgb__reg_lambda': 0.7491717064284391, 'xgb__reg_alpha': 0.42506753422232135, 'xgb__scale_pos_weight': 3.2945364391239647}\n",
      "Rank 8 | F1=0.9442 | Recall=0.9402 | Acc=0.9951 | Params={'smote__sampling_strategy': 0.1506662415897263, 'smote__k_neighbors': 5, 'xgb__n_estimators': 749, 'xgb__learning_rate': 0.1466044022919137, 'xgb__max_depth': 8, 'xgb__min_child_weight': 5, 'xgb__subsample': 0.8596388352295404, 'xgb__colsample_bytree': 0.6003412408537206, 'xgb__gamma': 0.17738524290110003, 'xgb__reg_lambda': 0.8793400855645954, 'xgb__reg_alpha': 0.24432824229826894, 'xgb__scale_pos_weight': 3.622805987134144}\n",
      "Rank 9 | F1=0.9430 | Recall=0.9405 | Acc=0.9949 | Params={'smote__sampling_strategy': 0.10284161210844245, 'smote__k_neighbors': 4, 'xgb__n_estimators': 525, 'xgb__learning_rate': 0.10066625419607746, 'xgb__max_depth': 8, 'xgb__min_child_weight': 6, 'xgb__subsample': 0.895079964474876, 'xgb__colsample_bytree': 0.6370342520385587, 'xgb__gamma': 0.0035021335886248756, 'xgb__reg_lambda': 0.5605493816118909, 'xgb__reg_alpha': 0.3227421954555426, 'xgb__scale_pos_weight': 4.031176311778868}\n",
      "Rank 10 | F1=0.9427 | Recall=0.9460 | Acc=0.9949 | Params={'smote__sampling_strategy': 0.15177781077559294, 'smote__k_neighbors': 3, 'xgb__n_estimators': 257, 'xgb__learning_rate': 0.07810490780002509, 'xgb__max_depth': 7, 'xgb__min_child_weight': 1, 'xgb__subsample': 0.9690386452850898, 'xgb__colsample_bytree': 0.7563749096362815, 'xgb__gamma': 0.05766192954395212, 'xgb__reg_lambda': 3.285509530406217, 'xgb__reg_alpha': 0.7203552143000452, 'xgb__scale_pos_weight': 3.1691113697155093}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 1) Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨\n",
    "# ======================\n",
    "class DatetimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        if \"date\" in df.columns and \"time\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(\n",
    "                df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n",
    "                errors=\"coerce\",\n",
    "                infer_datetime_format=True\n",
    "            )\n",
    "        df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "        df[\"shift\"] = df[\"hour\"].apply(lambda h: \"Day\" if 8 <= h <= 19 else \"Night\")\n",
    "        prev_count = df[\"count\"].iloc[0]\n",
    "        global_counts, accum = [], 0\n",
    "        for current_count in df[\"count\"]:\n",
    "            if current_count < prev_count:\n",
    "                accum += prev_count\n",
    "            global_counts.append(accum + current_count)\n",
    "            prev_count = current_count\n",
    "        df[\"global_count\"] = global_counts\n",
    "        df[\"year_month\"] = df[\"datetime\"].dt.to_period(\"M\")\n",
    "        df[\"monthly_count\"] = df.groupby(\"year_month\").cumcount() + 1\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[\"speed_ratio\"] = df[\"low_section_speed\"] / df[\"high_section_speed\"]\n",
    "        df[\"pressure_speed_ratio\"] = df[\"cast_pressure\"] / df[\"high_section_speed\"]\n",
    "        df.loc[(df[\"low_section_speed\"] == 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[(df[\"low_section_speed\"] != 0) & (df[\"high_section_speed\"] == 0), \"speed_ratio\"] = -1\n",
    "        df.loc[df[\"high_section_speed\"] == 0, \"pressure_speed_ratio\"] = -1\n",
    "        for col in [\"heating_furnace\", \"emergency_stop\", \"tryshot_signal\", \"EMS_operation_time\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "        if \"molten_temp\" in df.columns and df[\"molten_temp\"].isna().any():\n",
    "            df[\"molten_temp\"] = df[\"molten_temp\"].fillna(df[\"molten_temp\"].mode()[0])\n",
    "        if \"molten_volume\" in df.columns:\n",
    "            df[\"molten_volume\"] = df[\"molten_volume\"].interpolate(\"linear\").ffill().bfill()\n",
    "        df = df.replace([np.inf, -np.inf], -1)\n",
    "        return df\n",
    "\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_cols=None):\n",
    "        self.drop_cols = drop_cols or []\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=[c for c in self.drop_cols if c in X.columns])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2) Threshold Finder\n",
    "# ======================\n",
    "def find_best_threshold_fbeta(y_true, y_prob, beta=2.0):\n",
    "    p, r, t = precision_recall_curve(y_true, y_prob)\n",
    "    t = np.append(t, 1.0)\n",
    "    fbeta = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)\n",
    "    best_idx = int(np.nanargmax(fbeta))\n",
    "    return float(t[best_idx]), float(fbeta[best_idx])\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3) Main\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    test  = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "    y_train = train[\"passorfail\"]\n",
    "    X_train = train.drop(columns=[\"passorfail\"])\n",
    "    X_test  = test.copy()\n",
    "\n",
    "    # ----- Ïª§Ïä§ÌÖÄ Ï†ÑÏ≤òÎ¶¨ ÌõÑ Ïª¨Îüº Ï†ïÏùò\n",
    "    drop_cols = [\"id\",\"line\",\"name\",\"mold_name\",\"date\",\"time\",\"registration_time\",\n",
    "                 \"year_month\",\"hour\",\"datetime\",\"real_time\",\"working\"]\n",
    "\n",
    "    tmp_after = DatetimeFeatureExtractor().fit_transform(X_train)\n",
    "    tmp_after = FeatureEngineer().fit_transform(tmp_after)\n",
    "    tmp_after = DropColumns(drop_cols=drop_cols).fit_transform(tmp_after)\n",
    "\n",
    "    expected_cats = [\"mold_code\",\"heating_furnace\",\"EMS_operation_time\",\"shift\",\n",
    "                     \"emergency_stop\",\"tryshot_signal\"]\n",
    "    present_cats = [c for c in expected_cats if c in tmp_after.columns]\n",
    "\n",
    "    cat_pipe = SimpleImputer(strategy=\"most_frequent\")\n",
    "    num_pipe = SimpleImputer(strategy=\"median\")\n",
    "    num_selector = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "    model_preproc = ColumnTransformer(\n",
    "        transformers=[(\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), present_cats),\n",
    "                      (\"num\", num_pipe, num_selector)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    categorical_feature_indices = list(range(len(present_cats)))\n",
    "\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"datetime\", DatetimeFeatureExtractor()),\n",
    "        (\"engineer\", FeatureEngineer()),\n",
    "        (\"drop\", DropColumns(drop_cols=drop_cols)),\n",
    "        (\"prep\", model_preproc),\n",
    "        (\"smote\", SMOTENC(categorical_features=categorical_feature_indices, random_state=42)),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # ======================\n",
    "    # Optuna objective (F1 ÏµúÏ†ÅÌôî)\n",
    "    # ======================\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"smote__sampling_strategy\": trial.suggest_float(\"smote__sampling_strategy\", 0.1, 1.0),\n",
    "            \"smote__k_neighbors\": trial.suggest_int(\"smote__k_neighbors\", 3, 7),\n",
    "            \"xgb__n_estimators\": trial.suggest_int(\"xgb__n_estimators\", 200, 800),\n",
    "            \"xgb__learning_rate\": trial.suggest_float(\"xgb__learning_rate\", 0.01, 0.3),\n",
    "            \"xgb__max_depth\": trial.suggest_int(\"xgb__max_depth\", 3, 8),\n",
    "            \"xgb__min_child_weight\": trial.suggest_int(\"xgb__min_child_weight\", 1, 6),\n",
    "            \"xgb__subsample\": trial.suggest_float(\"xgb__subsample\", 0.6, 1.0),\n",
    "            \"xgb__colsample_bytree\": trial.suggest_float(\"xgb__colsample_bytree\", 0.6, 1.0),\n",
    "            \"xgb__gamma\": trial.suggest_float(\"xgb__gamma\", 0.0, 0.5),\n",
    "            \"xgb__reg_lambda\": trial.suggest_float(\"xgb__reg_lambda\", 0.0, 5.0),\n",
    "            \"xgb__reg_alpha\": trial.suggest_float(\"xgb__reg_alpha\", 0.0, 1.0),\n",
    "            \"xgb__scale_pos_weight\": trial.suggest_float(\"xgb__scale_pos_weight\", 1.0, 5.0),\n",
    "        }\n",
    "        pipe.set_params(**params)\n",
    "\n",
    "        # ÍµêÏ∞®Í≤ÄÏ¶ù Ï†êÏàò Í≥ÑÏÇ∞\n",
    "        f1     = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "        recall = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1).mean()\n",
    "        acc    = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "\n",
    "        trial.set_user_attr(\"recall\", recall)\n",
    "        trial.set_user_attr(\"accuracy\", acc)\n",
    "\n",
    "        return f1   # F1ÏùÑ ÏµúÏ†ÅÌôî Í∏∞Ï§ÄÏúºÎ°ú ÏÇ¨Ïö©\n",
    "\n",
    "    # ======================\n",
    "    # Optuna Ïã§Ìñâ\n",
    "    # ======================\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "    # ======================\n",
    "    # Top 10 Ï∂úÎ†• (F1 Í∏∞Ï§Ä)\n",
    "    # ======================\n",
    "    trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:10]\n",
    "    print(\"\\n===== Top 10 Trials (F1 Í∏∞Ï§Ä, Recall/Acc Ìè¨Ìï®) =====\")\n",
    "    for rank, t in enumerate(trials, 1):\n",
    "        print(f\"Rank {rank} | F1={t.value:.4f} | Recall={t.user_attrs['recall']:.4f} | \"\n",
    "              f\"Acc={t.user_attrs['accuracy']:.4f} | Params={t.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499edeb-0afe-4642-99bd-a9df6af3d35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'line', 'name', 'mold_name', 'time', 'date', 'count', 'working',\n",
       "       'emergency_stop', 'molten_temp', 'facility_operation_cycleTime',\n",
       "       'production_cycletime', 'low_section_speed', 'high_section_speed',\n",
       "       'molten_volume', 'cast_pressure', 'biscuit_thickness',\n",
       "       'upper_mold_temp1', 'upper_mold_temp2', 'upper_mold_temp3',\n",
       "       'lower_mold_temp1', 'lower_mold_temp2', 'lower_mold_temp3',\n",
       "       'sleeve_temperature', 'physical_strength', 'Coolant_temperature',\n",
       "       'EMS_operation_time', 'registration_time', 'tryshot_signal',\n",
       "       'mold_code', 'heating_furnace'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
